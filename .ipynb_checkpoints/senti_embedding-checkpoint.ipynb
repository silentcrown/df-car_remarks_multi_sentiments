{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision as tv\n",
    "import jieba\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch\n",
    "show = ToPILImage() # 可以把Tensor转成Image，方便可视化\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/df_sen_sub/train.csv')\n",
    "data = data.fillna('')\n",
    "senti = data['sentiment_value'].unique()\n",
    "content = list(data['content'])\n",
    "senti_word = list(data['sentiment_word'])\n",
    "senti_words = [senti_word[i]*100 for i in range(len(senti_word))]  #强调重点词\n",
    "test_data = pd.read_csv('../../data/df_sen_sub/test_public.csv')\n",
    "test_content = list(test_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.086 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b36a311ea3fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# 生成50维度的词向量模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrain_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../data/df_sen_sub/add_text.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 测试训练好的词向量模型，使用model[keyWord]即可获取keyword这个词的词向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b36a311ea3fc>\u001b[0m in \u001b[0;36mtrain_word2vec\u001b[0;34m(folder_path, size)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# worker是线程数量，建议与物理线程数量一致\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# min_count是指出现次数小于一定程度，就忽略，0表示不忽略\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# 训练结束就将模型保存起来\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('../../data/df_sen_sub/add_text.txt','w') as f:\n",
    "    f.write('\\n'.join(list(content) + list(senti_words) + list(test_content)))\n",
    "# 迭代器，使用jieba将句子进行分词\n",
    "class Sentences(object):# 这个类可以根据实际情况重写，我已经将所有的文章进行分句，并整合到了一个文件里面\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname # 句子所在文件，没句句子占一行\n",
    "        #jieba.load_userdict(\"wordBase.txt\") # 加载词库\n",
    "\n",
    "    def __iter__(self):\n",
    "        #for fname in os.listdir(self.dirname):\n",
    "        for line in open(self.dirname):\n",
    "                line = line.replace('\\n', '')\n",
    "                yield list(jieba.cut(line))\n",
    "\n",
    "sentences = []\n",
    "def train_word2vec(folder_path, size=100):\n",
    "    global sentences\n",
    "    sentences = Sentences(folder_path) #生成分词后的句子，是一个二维数组\n",
    "\n",
    "    # size是词向量长度\n",
    "    # worker是线程数量，建议与物理线程数量一致\n",
    "    # min_count是指出现次数小于一定程度，就忽略，0表示不忽略\n",
    "    model = Word2Vec(sentences, size=size, workers=8, min_count=0)\n",
    "\n",
    "    # 训练结束就将模型保存起来\n",
    "    model.save(\"../../data/df_sen_sub/add_word2vec_model\")\n",
    "\n",
    "# 生成50维度的词向量模型\n",
    "train_word2vec(\"../../data/df_sen_sub/add_text.txt\",100)\n",
    "\n",
    "# 测试训练好的词向量模型，使用model[keyWord]即可获取keyword这个词的词向量\n",
    "model = Word2Vec.load(\"../../data/df_sen_sub/add_word2vec_model\")\n",
    "sentences = list(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:38: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9947, 100, 100) (2364, 100, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9947,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vecs = []\n",
    "x_test_vecs = []  \n",
    "ind = 0\n",
    "word2ind = {}\n",
    "pretrained = []\n",
    "senti_sen = []\n",
    "senti_test_sen = []\n",
    "for i in range(len(sentences) - len(content)):\n",
    "    if i < len(content):\n",
    "        temp = []\n",
    "        t_s = []\n",
    "        for j in range(len(sentences[i])):\n",
    "            if sentences[i][j] not in word2ind:\n",
    "                pretrained.append(model[sentences[i][j]])\n",
    "                word2ind[sentences[i][j]] = ind\n",
    "                ind += 1\n",
    "            t_s.append(sentences[i][j])\n",
    "            temp.append(model[sentences[i][j]])\n",
    "        if len(sentences[i + len(content)]) != 0:\n",
    "            for j in range(len(sentences[i + len(content)])):\n",
    "                if sentences[i + len(content)][j] not in word2ind:\n",
    "                    word2ind[sentences[i + len(content)][j]] = ind\n",
    "                    pretrained.append(model[sentences[i + len(content)][j]])\n",
    "                    ind += 1\n",
    "                t_s.append(sentences[i + len(content)][j])\n",
    "                temp.append(model[sentences[i + len(content)][j]])\n",
    "        while len(temp) < 100:\n",
    "            temp.append([0.0] * 100)\n",
    "        if len(temp) > 100:\n",
    "            temp = temp[:100]\n",
    "        x_vecs.append(temp)\n",
    "        senti_sen.append(t_s[:100])\n",
    "    else:\n",
    "        temp = []\n",
    "        t_s = []\n",
    "        for j in range(len(sentences[i + len(content)])):\n",
    "            if sentences[i + len(content)][j] not in word2ind:\n",
    "                pretrained.append(model[sentences[i + len(content)][j]])\n",
    "                word2ind[sentences[i + len(content)][j]] = ind\n",
    "                ind += 1\n",
    "            t_s.append(sentences[i + len(content)][j])\n",
    "            temp.append(model[sentences[i + len(content)][j]])\n",
    "        while len(temp) < 100:\n",
    "            temp.append([0.0] * 100)\n",
    "        if len(temp) > 100:\n",
    "            temp = temp[:100]\n",
    "        x_test_vecs.append(temp)\n",
    "        senti_test_sen.append(t_s[:100])     \n",
    "x_vecs = np.array(x_vecs)\n",
    "x_test_vecs = np.array(x_test_vecs)\n",
    "print(x_vecs.shape, x_test_vecs.shape)\n",
    "\n",
    "y_map = []\n",
    "map_ = [0, -1, 1]\n",
    "y = list(data['sentiment_value'])\n",
    "for i in range(len(y)):\n",
    "    y_map.append(map_.index(y[i]))\n",
    "y_map = np.array(y_map)\n",
    "y_map.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_vecs\n",
    "X_test = x_test_vecs\n",
    "y = y_map\n",
    "pretrained = pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pretrained, open('../../data/pretrained.txt', 'wb'))\n",
    "pickle.dump(word2ind, open('../../data/word2ind.txt', 'wb'))\n",
    "pickle.dump(senti_test_sen, open('../../data/senti_test_sen.txt', 'wb'))\n",
    "pickle.dump(senti_sen, open('../../data/senti_sen.txt', 'wb'))\n",
    "pickle.dump(X, open('../../data/sentiment_x.txt', 'wb'))\n",
    "pickle.dump(X_test, open('../../data/sentiment_x_test.txt', 'wb'))\n",
    "pickle.dump(y, open('../../data/sentiment_y.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "word2ind = pickle.load(open('../../data/word2ind.txt', 'rb'))\n",
    "senti_test_sen = pickle.load(open('../../data/senti_test_sen.txt', 'rb'))\n",
    "senti_sen = pickle.load(open('../../data/senti_sen.txt', 'rb'))\n",
    "X = pickle.load(open('../../data/sentiment_x.txt', 'rb'))\n",
    "X_test = pickle.load(open('../../data/sentiment_x_test.txt', 'rb'))\n",
    "y = pickle.load(open('../../data/sentiment_y.txt', 'rb'))\n",
    "pretrained = pickle.load(open('../../data/pretrained.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.append([0.0] * 100)\n",
    "word2ind['null'] = 17387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = np.array(pretrained)\n",
    "han_x = []\n",
    "for i in range(len(senti_sen)):\n",
    "    temp = []\n",
    "    for j in range(len(senti_sen[i])):\n",
    "        temp.append(word2ind[senti_sen[i][j]])\n",
    "    while len(temp) < 100:\n",
    "        temp.append(17387)\n",
    "\n",
    "    temp = temp[:100]\n",
    "\n",
    "    han_x.append(temp)\n",
    "han_x = np.array(han_x)\n",
    "\n",
    "han_x_test = []\n",
    "for i in range(len(senti_test_sen)):\n",
    "    temp = []\n",
    "    for j in range(len(senti_test_sen[i])):\n",
    "        temp.append(word2ind[senti_test_sen[i][j]])\n",
    "    while len(temp) < 100:\n",
    "        temp.append(17387)\n",
    "\n",
    "    temp = temp[:100]\n",
    "\n",
    "    han_x_test.append(temp)\n",
    "han_x = np.array(han_x)\n",
    "han_x_test = np.array(han_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x, test_x, train_y, test_y = train_test_split(han_x, y, test_size=0.2, random_state=42)\n",
    "train_x = han_x\n",
    "train_y = y\n",
    "test_x = han_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9947, 9947, 2364)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x), len(train_y), len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [(train_x[i], train_y[i]) for i in range(len(train_x))]\n",
    "testset = [(test_x[i]) for i in range(len(test_x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=4,\n",
    "                    shuffle=True, \n",
    "                    num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "                    testset, \n",
    "                    batch_size=4,\n",
    "                    shuffle=False, \n",
    "                    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fca5c3add68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (rnn1): RNN(100, 100)\n",
      "  (fc1): Linear(in_features=2500, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (embeddings): Embedding(19901, 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pretrained, vocab_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn1 = nn.RNN(100, 100) \n",
    "        self.fc1   = nn.Linear(50 * 50, 120) \n",
    "        self.fc2   = nn.Linear(120, 50)\n",
    "        self.fc3   = nn.Linear(50, 3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, 100)\n",
    "        pretrained_weight = np.array(pretrained)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.embeddings(x)\n",
    "        output, hn =  self.rnn1(x.float())\n",
    "        x = F.max_pool2d(F.relu(output), (2, 2))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)         \n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net(pretrained, len(pretrained))\n",
    "print(net)\n",
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.873\n",
      "[1,   400] loss: 0.747\n",
      "[1,   600] loss: 0.744\n",
      "[1,   800] loss: 0.677\n",
      "[1,  1000] loss: 0.716\n",
      "[1,  1200] loss: 0.687\n",
      "[1,  1400] loss: 0.667\n",
      "[1,  1600] loss: 0.752\n",
      "[1,  1800] loss: 0.609\n",
      "[1,  2000] loss: 0.653\n",
      "[1,  2200] loss: 0.668\n",
      "[1,  2400] loss: 0.671\n",
      "[2,   200] loss: 0.653\n",
      "[2,   400] loss: 0.638\n",
      "[2,   600] loss: 0.619\n",
      "[2,   800] loss: 0.595\n",
      "[2,  1000] loss: 0.565\n",
      "[2,  1200] loss: 0.638\n",
      "[2,  1400] loss: 0.666\n",
      "[2,  1600] loss: 0.648\n",
      "[2,  1800] loss: 0.651\n",
      "[2,  2000] loss: 0.584\n",
      "[2,  2200] loss: 0.613\n",
      "[2,  2400] loss: 0.626\n",
      "[3,   200] loss: 0.554\n",
      "[3,   400] loss: 0.603\n",
      "[3,   600] loss: 0.597\n",
      "[3,   800] loss: 0.594\n",
      "[3,  1000] loss: 0.636\n",
      "[3,  1200] loss: 0.605\n",
      "[3,  1400] loss: 0.612\n",
      "[3,  1600] loss: 0.607\n",
      "[3,  1800] loss: 0.608\n",
      "[3,  2000] loss: 0.543\n",
      "[3,  2200] loss: 0.560\n",
      "[3,  2400] loss: 0.598\n",
      "[4,   200] loss: 0.590\n",
      "[4,   400] loss: 0.536\n",
      "[4,   600] loss: 0.570\n",
      "[4,   800] loss: 0.566\n",
      "[4,  1000] loss: 0.603\n",
      "[4,  1200] loss: 0.586\n",
      "[4,  1400] loss: 0.522\n",
      "[4,  1600] loss: 0.580\n",
      "[4,  1800] loss: 0.593\n",
      "[4,  2000] loss: 0.587\n",
      "[4,  2200] loss: 0.569\n",
      "[4,  2400] loss: 0.543\n",
      "[5,   200] loss: 0.582\n",
      "[5,   400] loss: 0.498\n",
      "[5,   600] loss: 0.547\n",
      "[5,   800] loss: 0.533\n",
      "[5,  1000] loss: 0.568\n",
      "[5,  1200] loss: 0.551\n",
      "[5,  1400] loss: 0.550\n",
      "[5,  1600] loss: 0.559\n",
      "[5,  1800] loss: 0.564\n",
      "[5,  2000] loss: 0.530\n",
      "[5,  2200] loss: 0.538\n",
      "[5,  2400] loss: 0.553\n",
      "[6,   200] loss: 0.484\n",
      "[6,   400] loss: 0.534\n",
      "[6,   600] loss: 0.547\n",
      "[6,   800] loss: 0.550\n",
      "[6,  1000] loss: 0.514\n",
      "[6,  1200] loss: 0.516\n",
      "[6,  1400] loss: 0.512\n",
      "[6,  1600] loss: 0.539\n",
      "[6,  1800] loss: 0.503\n",
      "[6,  2000] loss: 0.482\n",
      "[6,  2200] loss: 0.519\n",
      "[6,  2400] loss: 0.561\n",
      "[7,   200] loss: 0.502\n",
      "[7,   400] loss: 0.469\n",
      "[7,   600] loss: 0.491\n",
      "[7,   800] loss: 0.509\n",
      "[7,  1000] loss: 0.495\n",
      "[7,  1200] loss: 0.474\n",
      "[7,  1400] loss: 0.470\n",
      "[7,  1600] loss: 0.537\n",
      "[7,  1800] loss: 0.548\n",
      "[7,  2000] loss: 0.517\n",
      "[7,  2200] loss: 0.493\n",
      "[7,  2400] loss: 0.496\n",
      "[8,   200] loss: 0.452\n",
      "[8,   400] loss: 0.459\n",
      "[8,   600] loss: 0.431\n",
      "[8,   800] loss: 0.485\n",
      "[8,  1000] loss: 0.496\n",
      "[8,  1200] loss: 0.521\n",
      "[8,  1400] loss: 0.472\n",
      "[8,  1600] loss: 0.510\n",
      "[8,  1800] loss: 0.458\n",
      "[8,  2000] loss: 0.446\n",
      "[8,  2200] loss: 0.499\n",
      "[8,  2400] loss: 0.494\n",
      "[9,   200] loss: 0.402\n",
      "[9,   400] loss: 0.442\n",
      "[9,   600] loss: 0.479\n",
      "[9,   800] loss: 0.454\n",
      "[9,  1000] loss: 0.488\n",
      "[9,  1200] loss: 0.523\n",
      "[9,  1400] loss: 0.398\n",
      "[9,  1600] loss: 0.437\n",
      "[9,  1800] loss: 0.430\n",
      "[9,  2000] loss: 0.392\n",
      "[9,  2200] loss: 0.517\n",
      "[9,  2400] loss: 0.481\n",
      "[10,   200] loss: 0.420\n",
      "[10,   400] loss: 0.432\n",
      "[10,   600] loss: 0.390\n",
      "[10,   800] loss: 0.430\n",
      "[10,  1000] loss: 0.417\n",
      "[10,  1200] loss: 0.424\n",
      "[10,  1400] loss: 0.462\n",
      "[10,  1600] loss: 0.461\n",
      "[10,  1800] loss: 0.429\n",
      "[10,  2000] loss: 0.467\n",
      "[10,  2200] loss: 0.424\n",
      "[10,  2400] loss: 0.395\n",
      "[11,   200] loss: 0.395\n",
      "[11,   400] loss: 0.380\n",
      "[11,   600] loss: 0.437\n",
      "[11,   800] loss: 0.377\n",
      "[11,  1000] loss: 0.415\n",
      "[11,  1200] loss: 0.371\n",
      "[11,  1400] loss: 0.382\n",
      "[11,  1600] loss: 0.412\n",
      "[11,  1800] loss: 0.414\n",
      "[11,  2000] loss: 0.413\n",
      "[11,  2200] loss: 0.415\n",
      "[11,  2400] loss: 0.410\n",
      "[12,   200] loss: 0.344\n",
      "[12,   400] loss: 0.338\n",
      "[12,   600] loss: 0.340\n",
      "[12,   800] loss: 0.362\n",
      "[12,  1000] loss: 0.392\n",
      "[12,  1200] loss: 0.350\n",
      "[12,  1400] loss: 0.375\n",
      "[12,  1600] loss: 0.385\n",
      "[12,  1800] loss: 0.403\n",
      "[12,  2000] loss: 0.430\n",
      "[12,  2200] loss: 0.341\n",
      "[12,  2400] loss: 0.415\n",
      "[13,   200] loss: 0.285\n",
      "[13,   400] loss: 0.323\n",
      "[13,   600] loss: 0.318\n",
      "[13,   800] loss: 0.377\n",
      "[13,  1000] loss: 0.364\n",
      "[13,  1200] loss: 0.349\n",
      "[13,  1400] loss: 0.332\n",
      "[13,  1600] loss: 0.315\n",
      "[13,  1800] loss: 0.390\n",
      "[13,  2000] loss: 0.312\n",
      "[13,  2200] loss: 0.379\n",
      "[13,  2400] loss: 0.358\n",
      "[14,   200] loss: 0.267\n",
      "[14,   400] loss: 0.282\n",
      "[14,   600] loss: 0.318\n",
      "[14,   800] loss: 0.286\n",
      "[14,  1000] loss: 0.316\n",
      "[14,  1200] loss: 0.340\n",
      "[14,  1400] loss: 0.336\n",
      "[14,  1600] loss: 0.305\n",
      "[14,  1800] loss: 0.310\n",
      "[14,  2000] loss: 0.364\n",
      "[14,  2200] loss: 0.369\n",
      "[14,  2400] loss: 0.343\n",
      "[15,   200] loss: 0.273\n",
      "[15,   400] loss: 0.266\n",
      "[15,   600] loss: 0.250\n",
      "[15,   800] loss: 0.355\n",
      "[15,  1000] loss: 0.304\n",
      "[15,  1200] loss: 0.278\n",
      "[15,  1400] loss: 0.290\n",
      "[15,  1600] loss: 0.308\n",
      "[15,  1800] loss: 0.273\n",
      "[15,  2000] loss: 0.299\n",
      "[15,  2200] loss: 0.300\n",
      "[15,  2400] loss: 0.281\n",
      "[16,   200] loss: 0.249\n",
      "[16,   400] loss: 0.222\n",
      "[16,   600] loss: 0.250\n",
      "[16,   800] loss: 0.259\n",
      "[16,  1000] loss: 0.226\n",
      "[16,  1200] loss: 0.233\n",
      "[16,  1400] loss: 0.266\n",
      "[16,  1600] loss: 0.243\n",
      "[16,  1800] loss: 0.303\n",
      "[16,  2000] loss: 0.217\n",
      "[16,  2200] loss: 0.288\n",
      "[16,  2400] loss: 0.281\n",
      "[17,   200] loss: 0.206\n",
      "[17,   400] loss: 0.192\n",
      "[17,   600] loss: 0.179\n",
      "[17,   800] loss: 0.209\n",
      "[17,  1000] loss: 0.200\n",
      "[17,  1200] loss: 0.238\n",
      "[17,  1400] loss: 0.256\n",
      "[17,  1600] loss: 0.254\n",
      "[17,  1800] loss: 0.231\n",
      "[17,  2000] loss: 0.225\n",
      "[17,  2200] loss: 0.270\n",
      "[17,  2400] loss: 0.244\n",
      "[18,   200] loss: 0.171\n",
      "[18,   400] loss: 0.170\n",
      "[18,   600] loss: 0.182\n",
      "[18,   800] loss: 0.202\n",
      "[18,  1000] loss: 0.159\n",
      "[18,  1200] loss: 0.193\n",
      "[18,  1400] loss: 0.219\n",
      "[18,  1600] loss: 0.199\n",
      "[18,  1800] loss: 0.204\n",
      "[18,  2000] loss: 0.269\n",
      "[18,  2200] loss: 0.242\n",
      "[18,  2400] loss: 0.240\n",
      "[19,   200] loss: 0.165\n",
      "[19,   400] loss: 0.155\n",
      "[19,   600] loss: 0.196\n",
      "[19,   800] loss: 0.178\n",
      "[19,  1000] loss: 0.201\n",
      "[19,  1200] loss: 0.168\n",
      "[19,  1400] loss: 0.178\n",
      "[19,  1600] loss: 0.180\n",
      "[19,  1800] loss: 0.159\n",
      "[19,  2000] loss: 0.247\n",
      "[19,  2200] loss: 0.201\n",
      "[19,  2400] loss: 0.239\n",
      "[20,   200] loss: 0.148\n",
      "[20,   400] loss: 0.141\n",
      "[20,   600] loss: 0.162\n",
      "[20,   800] loss: 0.202\n",
      "[20,  1000] loss: 0.128\n",
      "[20,  1200] loss: 0.136\n",
      "[20,  1400] loss: 0.164\n",
      "[20,  1600] loss: 0.159\n",
      "[20,  1800] loss: 0.174\n",
      "[20,  2000] loss: 0.188\n",
      "[20,  2200] loss: 0.202\n",
      "[20,  2400] loss: 0.213\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(8)\n",
    "for epoch in range(20):  \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # 输入数据\n",
    "        inputs, labels = data\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()   \n",
    "        \n",
    "        # 更新参数 \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印log信息\n",
    "        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199: # 每200个batch打印一下训练状态\n",
    "            print('[%d, %5d] loss: %.3f' \\\n",
    "                  % (epoch+1, i+1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fc9670562b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/queues.py\", line 345, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "#images, labels = dataiter.next() # 一个batch返回4张图片\n",
    "images = dataiter.next() # 一个batch返回4张图片\n",
    "#print('实际的label: ', ' '.join(\\\n",
    "            #'%08s'%map_[labels[j]] for j in range(4)))\n",
    "\n",
    "# 计算图片在每个类别上的分数\n",
    "outputs = net(images)\n",
    "# 得分最高的那个类\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#print('预测结果: ', ' '.join('%5s'\\\n",
    "            #% map_[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0 # 预测正确的图片数\n",
    "total = 0 # 总共的图片数\n",
    "\n",
    "\n",
    "# 由于测试的时候不需要求导，可以暂时关闭autograd，提高速度，节约内存\n",
    "fake = {}\n",
    "text = []\n",
    "vec = []\n",
    "l = []\n",
    "p = []\n",
    "res = []\n",
    "map_ = [0, -1, 1]\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images = data#images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        res = res + [map_[int(predicted[i])] for i in range(len(predicted))]\n",
    "        #for j in range(len(predicted)):\n",
    "            #if predicted[j] != labels[j]:\n",
    "                #vec.append(images[j])\n",
    "                #l.append(map_[int(labels[j])])\n",
    "                #p.append(map_[int(predicted[j])])\n",
    "        #total += labels.size(0)\n",
    "        #correct += (predicted == labels).sum()\n",
    "\n",
    "#print('3283张测试集中的准确率为: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2word = {}\n",
    "for key in word2ind:\n",
    "    ind2word[word2ind[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'因为'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vec)):\n",
    "    temp = vec[i]\n",
    "    tt = []\n",
    "    for j in range(len(temp)):\n",
    "        tt.append(ind2word[int(temp[j])])\n",
    "    text.append(''.join(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2364"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = {}\n",
    "content_id,subject,sentiment_value,sentiment_word\n",
    "fake['text'] = test_content\n",
    "fake['sentiment_value'] = res\n",
    "df = pd.DataFrame(fake)\n",
    "df.to_csv('../../data/df_sen_sub/senti_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>欧蓝德，价格便宜，森林人太贵啦！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>楼主什么时候提的车，南昌优惠多少啊</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>吉林，2.5优惠20000，送三年九次保养，贴膜</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>便宜2万的豪华特装，实用配制提升，优惠还给力，确实划算。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>如果实在想买就等车展期间，优惠2万，我24.98万入的2.5豪</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0时尚优惠两万现金吗？还有其他赠品吗？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27.5 相比较优惠的少</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>综合优惠两万，不是现金优惠两万，送的垃圾东西都包含在内.大忽悠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>差不多15000左右的优惠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>恭喜恭喜，这个配置性价比很高</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>谁还用车载导航呀！用手机导航吧及时性好！而且到港后装的机头不行，在意的话建议去换一个机头顺便...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>你好，我也是受够了16款的垃圾导航。这个美版主机哪里可以买到？安装后可以完美使用吗，有什么问...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>森林人导航确实垃圾。再好的车再好的导航都没有手机导航好用，我提车后只用了二个多月的车载导航就...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>导航现在都用手机的，音箱没有说的那么差，就是低音差点，人声其实还可以。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>黑屏经常发生啊和手机蓝牙，连接后电话打进来根本听不到对方在说什么，想指往导航功能就会让你上火</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>传感器是在轮毂里面的，连着气门嘴的，跟轮胎没关系</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>60肯定是不行的。 原车轮胎的直径=215*0.65*2 15*25.4（一英寸=25.4毫...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>那你们换的什么中控呀？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>都这样，提高油号有所减轻。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>你的真费油，我在北京开才9.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>接孩子应该就在城区，这油耗正常的。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>平路上定速８０就是５个左右，我新车１０００公里内都是这样跑的，可以的。高速不过１００会更低，...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>俩车排量都不行，1.5t昂科威动力不行，比2.0t还费油，你图什么？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>我13款2.5的2w公里就用30了 。 也没见什么不妥，就是油耗高一点而已。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2000公里，平均油耗9.8的飘过~~~</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>换轮胎应当根据个人用车情况选择，里程少的应偏重舒适性，里程多的偏重轮胎的耐磨指标。舒适性好的...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>全车很多电器件都是松下的包括很多继电器，转向机是日本日立的，发电机是三菱的，空调压缩机是法雷...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>除开这四点 森林人还有很多缺点 比如内饰做工廉价感十足 高速上了120噪音大 CVT牺牲驾驶...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>真皮座椅真的不用加套。我的用了5年了，还是很好的。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>换汤不换药，没感觉。安全设备肯定会阉割的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>冬天车水温没上去80，开空调也会这样的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>上车千万别开暖风，全部关闭，温度调到最低。走个6，7分钟蓝灯没了再开暖风，走走停停，D档刹车...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>我也觉得方向盘有些虚位，回正力度不足，路感不好。和老2.5的液压助力差距不小。CVT变速箱还...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>总觉得棕色太商务，不过配银色车身倒是别有一番味道！恭喜??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>10款以前发动机加95号汽油，11款之后发动机改进后加92号汽油，但11款以后发动机出现了高...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>这台发动机没听说过烧机油啊，我刚刚提了台SE11万公里，目前跑了4000公里没有发现烧机油</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>听说米其林的胎噪音低，我也正在考虑</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>我就是直接旋转中间的钮 开空调 默认吹脚 压缩机也好像工作 ac 没开</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>是的，看动力选择和设计取向了，希望森林人还是那个森林人。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>以前都是拼发动机，现在都是拼变速箱，谁有好的变速箱谁就卖得好。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>斯巴鲁的中控屏都不是原车的，原车的屏进中国就被庞大集团换了一个山寨的屏。所以不理解他们用那个...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>看来这是老款森林人又一个做工上的问题！我的同样主驾驶屁股不热后背热，并且有其它车友遇到过这个...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>外观不要改太多，硬派些</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>不管哪年的产物人家也是Q5底盘规格布局比森林人强得不是一星半点，就算傲虎各方面也没有资格去挑...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>正解。目前大家的油耗大部分都是0-20的数据。属于最省油的机油了。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>2.5的，双排会丢失动力吗？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>转向助力 空调 发电机 真空泵需要隔减速箱？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>我的是16款2.0，也快2万公里了，油耗8.6左右。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>有呀，哈哈。雪地里，方向盘打死，加油的话，确实抖。不过是后轮抖，但是前当的雪咋办？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>可以设置的，是不是碰到中控屏膜了</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>cvt这油耗不正常。at也就12撑死</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>对一分钱一分货。换位思考人车好好的可能特便宜卖你吗。二手车最忌讳贪便宜。正经车商的车都是有一...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>我用的是佳明导航，跑长途的时候挂上，市内跑的时候取下来。方便实用。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>水温灯灭了以后。感觉变速箱有保护，不增档</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>12款2.5xs  4at， 目前行驶10万公里。  这个车除了机油消耗大点， 能拉货， 能...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>我的也是斯巴鲁，你别踩刹车，按开关就断了，</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>全时四驱仅比一般SUV车强一点，肯定干不过Q5，XC60，连A4也干不过。水平对置仅体现在低...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>哈哈，终于看到有人开始厌烦前置雷达的声音了，这个亲，那个声音来自哪里？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362</th>\n",
       "      <td>请教一下，变速箱油，差速器油，火花塞，分别多久更换。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>求购二手１４款ＸＴ的后刹车总成。（已网购到手了）</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  predict\n",
       "0                                  欧蓝德，价格便宜，森林人太贵啦！            0\n",
       "1                                     楼主什么时候提的车，南昌优惠多少啊        0\n",
       "2                              吉林，2.5优惠20000，送三年九次保养，贴膜        0\n",
       "3                          便宜2万的豪华特装，实用配制提升，优惠还给力，确实划算。        0\n",
       "4                       如果实在想买就等车展期间，优惠2万，我24.98万入的2.5豪        0\n",
       "5                             2.0时尚优惠两万现金吗？还有其他赠品吗？            0\n",
       "6                                          27.5 相比较优惠的少        0\n",
       "7                  综合优惠两万，不是现金优惠两万，送的垃圾东西都包含在内.大忽悠.            0\n",
       "8                                     差不多15000左右的优惠            0\n",
       "9                                        恭喜恭喜，这个配置性价比很高        0\n",
       "10    谁还用车载导航呀！用手机导航吧及时性好！而且到港后装的机头不行，在意的话建议去换一个机头顺便...        0\n",
       "11    你好，我也是受够了16款的垃圾导航。这个美版主机哪里可以买到？安装后可以完美使用吗，有什么问...        0\n",
       "12    森林人导航确实垃圾。再好的车再好的导航都没有手机导航好用，我提车后只用了二个多月的车载导航就...        0\n",
       "13                  导航现在都用手机的，音箱没有说的那么差，就是低音差点，人声其实还可以。        0\n",
       "14       黑屏经常发生啊和手机蓝牙，连接后电话打进来根本听不到对方在说什么，想指往导航功能就会让你上火        0\n",
       "15                             传感器是在轮毂里面的，连着气门嘴的，跟轮胎没关系        0\n",
       "16    60肯定是不行的。 原车轮胎的直径=215*0.65*2 15*25.4（一英寸=25.4毫...        0\n",
       "17                                      那你们换的什么中控呀？            0\n",
       "18                                        都这样，提高油号有所减轻。        0\n",
       "19                                      你的真费油，我在北京开才9.2        0\n",
       "20                                    接孩子应该就在城区，这油耗正常的。        0\n",
       "21    平路上定速８０就是５个左右，我新车１０００公里内都是这样跑的，可以的。高速不过１００会更低，...        0\n",
       "22               俩车排量都不行，1.5t昂科威动力不行，比2.0t还费油，你图什么？            0\n",
       "23           我13款2.5的2w公里就用30了 。 也没见什么不妥，就是油耗高一点而已。            0\n",
       "24                                 2000公里，平均油耗9.8的飘过~~~        0\n",
       "26    换轮胎应当根据个人用车情况选择，里程少的应偏重舒适性，里程多的偏重轮胎的耐磨指标。舒适性好的...        0\n",
       "28    全车很多电器件都是松下的包括很多继电器，转向机是日本日立的，发电机是三菱的，空调压缩机是法雷...        0\n",
       "29    除开这四点 森林人还有很多缺点 比如内饰做工廉价感十足 高速上了120噪音大 CVT牺牲驾驶...        0\n",
       "31                            真皮座椅真的不用加套。我的用了5年了，还是很好的。        0\n",
       "32                                 换汤不换药，没感觉。安全设备肯定会阉割的        0\n",
       "...                                                 ...      ...\n",
       "2329                                冬天车水温没上去80，开空调也会这样的        0\n",
       "2330  上车千万别开暖风，全部关闭，温度调到最低。走个6，7分钟蓝灯没了再开暖风，走走停停，D档刹车...        0\n",
       "2331  我也觉得方向盘有些虚位，回正力度不足，路感不好。和老2.5的液压助力差距不小。CVT变速箱还...        0\n",
       "2334                      总觉得棕色太商务，不过配银色车身倒是别有一番味道！恭喜??        0\n",
       "2335  10款以前发动机加95号汽油，11款之后发动机改进后加92号汽油，但11款以后发动机出现了高...        0\n",
       "2336      这台发动机没听说过烧机油啊，我刚刚提了台SE11万公里，目前跑了4000公里没有发现烧机油        0\n",
       "2337                                  听说米其林的胎噪音低，我也正在考虑        0\n",
       "2339                我就是直接旋转中间的钮 开空调 默认吹脚 压缩机也好像工作 ac 没开        0\n",
       "2340                       是的，看动力选择和设计取向了，希望森林人还是那个森林人。        0\n",
       "2341                    以前都是拼发动机，现在都是拼变速箱，谁有好的变速箱谁就卖得好。        0\n",
       "2343  斯巴鲁的中控屏都不是原车的，原车的屏进中国就被庞大集团换了一个山寨的屏。所以不理解他们用那个...        0\n",
       "2344  看来这是老款森林人又一个做工上的问题！我的同样主驾驶屁股不热后背热，并且有其它车友遇到过这个...        0\n",
       "2345                                        外观不要改太多，硬派些        0\n",
       "2346  不管哪年的产物人家也是Q5底盘规格布局比森林人强得不是一星半点，就算傲虎各方面也没有资格去挑...        0\n",
       "2347                  正解。目前大家的油耗大部分都是0-20的数据。属于最省油的机油了。        0\n",
       "2348                                     2.5的，双排会丢失动力吗？        0\n",
       "2349                             转向助力 空调 发电机 真空泵需要隔减速箱？        0\n",
       "2350                         我的是16款2.0，也快2万公里了，油耗8.6左右。        0\n",
       "2351          有呀，哈哈。雪地里，方向盘打死，加油的话，确实抖。不过是后轮抖，但是前当的雪咋办？        0\n",
       "2352                                   可以设置的，是不是碰到中控屏膜了        0\n",
       "2353                                 cvt这油耗不正常。at也就12撑死        0\n",
       "2354  对一分钱一分货。换位思考人车好好的可能特便宜卖你吗。二手车最忌讳贪便宜。正经车商的车都是有一...        0\n",
       "2355                  我用的是佳明导航，跑长途的时候挂上，市内跑的时候取下来。方便实用。        0\n",
       "2356                               水温灯灭了以后。感觉变速箱有保护，不增档        0\n",
       "2357  12款2.5xs  4at， 目前行驶10万公里。  这个车除了机油消耗大点， 能拉货， 能...        0\n",
       "2358                              我的也是斯巴鲁，你别踩刹车，按开关就断了，        0\n",
       "2360  全时四驱仅比一般SUV车强一点，肯定干不过Q5，XC60，连A4也干不过。水平对置仅体现在低...        0\n",
       "2361                哈哈，终于看到有人开始厌烦前置雷达的声音了，这个亲，那个声音来自哪里？        0\n",
       "2362                         请教一下，变速箱油，差速器油，火花塞，分别多久更换。        0\n",
       "2363                           求购二手１４款ＸＴ的后刹车总成。（已网购到手了）        0\n",
       "\n",
       "[2105 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['predict'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yxvenv]",
   "language": "python",
   "name": "conda-env-yxvenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
