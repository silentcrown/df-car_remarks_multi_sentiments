{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision as tv\n",
    "import jieba\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch as t\n",
    "show = ToPILImage() # 可以把Tensor转成Image，方便可视化\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/df_sen_sub/train.csv')\n",
    "subs = data['subject'].unique()\n",
    "content = data['content'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.001 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/df_sen_sub/text.txt','w') as f:\n",
    "    f.write('\\n'.join(list(data['content'])))\n",
    "# 迭代器，使用jieba将句子进行分词\n",
    "class Sentences(object):# 这个类可以根据实际情况重写，我已经将所有的文章进行分句，并整合到了一个文件里面\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname # 句子所在文件，没句句子占一行\n",
    "        #jieba.load_userdict(\"wordBase.txt\") # 加载词库\n",
    "\n",
    "    def __iter__(self):\n",
    "        #for fname in os.listdir(self.dirname):\n",
    "        for line in open(self.dirname):\n",
    "                yield list(jieba.cut(line))\n",
    "\n",
    "sentences = []\n",
    "def train_word2vec(folder_path, size=100):\n",
    "    global sentences\n",
    "    sentences = Sentences(folder_path) #生成分词后的句子，是一个二维数组\n",
    "\n",
    "    # size是词向量长度\n",
    "    # worker是线程数量，建议与物理线程数量一致\n",
    "    # min_count是指出现次数小于一定程度，就忽略，0表示不忽略\n",
    "    model = Word2Vec(sentences, size=size, workers=8, min_count=0)\n",
    "\n",
    "    # 训练结束就将模型保存起来\n",
    "    model.save(\"../../data/df_sen_sub/word2vec_model\")\n",
    "\n",
    "# 生成50维度的词向量模型\n",
    "train_word2vec(\"../../data/df_sen_sub/text.txt\",50)\n",
    "\n",
    "# 测试训练好的词向量模型，使用model[keyWord]即可获取keyword这个词的词向量\n",
    "model = Word2Vec.load(\"../../data/df_sen_sub/word2vec_model\")\n",
    "sentences = list(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9947, 50, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>斯柯达要说质量，似乎比大众要好一点，价格也低一些，用料完全一样。我听说过野帝，但没听说过你说...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>这玩意都是给有钱任性又不懂车的土豪用的，这价格换一次我妹夫EP020可以换三锅了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>17价格忒高，估计也就是14-15左右。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>我开始就是荣放2.5  森林人2.5二选一    荣放主要是底盘质感不行   太硬  其次是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>唉，这货的价格死硬死硬的，低配版优惠1万据说已经罕有了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>价格的话只能说一般般吧，太仓前段时间定的比你便宜！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>听过，价格太贵，但一直念念不忘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>恭喜恭喜，这个优惠不错哦！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>优惠幅度不小了，北京优惠八千</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>优惠可以了！购进吧！买了不会后悔的！时间可鉴！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>现在 什么价 优惠多少</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>优惠一万一 送贴膜装甲脚垫 铁西庞大</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>我也大连的，最近也考虑入手森林人，优惠太小了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>山东威海全系这才优惠3000，MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>下手了，豪导特供，优惠1.6万</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>优惠了8000，什么都不送。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>兄弟2.5豪导特供优惠1.6万可以下手吗？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>恭喜，优惠多少，我准备明天去提，2.5特装优惠两万</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>什么车款，多少优惠啊？康桥我只能谈到优惠1.5其他还送什么没问，试驾了一次看了两次2.0时尚...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>优惠1 ，就送行程记录</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>平常心吧，我本来想买傲虎，优惠不大，钱不够，改提森林人，6月提尊贵优惠1.4w，到了9月做活...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>哈哈，25.48。优惠够给力</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>兄弟，我也云南的，你的优惠也挺大的，现在哪里来的全系3万啊。。。现傲虎全系无优惠，森林人才八...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>来湖北，我上月订的2.0蓝色时尚，优惠1万3送6次保养，本月底提车</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>什么都没有，就是现金优惠和给了点小东西</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>什么都没有，就是现金优惠和给了点小东西，你最好先问问看有没有车</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>原来最低也就优惠15000至16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>我们这里，优惠2000，我呵呵了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917</th>\n",
       "      <td>3</td>\n",
       "      <td>风噪大 音响差驾驶什么的，秒杀其他</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918</th>\n",
       "      <td>7</td>\n",
       "      <td>安全大于一切，出远门前一定要检查车子，主要有全车油水，轮胎，刹车系统等。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9919</th>\n",
       "      <td>5</td>\n",
       "      <td>发动机直接就不一样了 FA20是斯巴鲁的招牌发动机 给WRX用要是烧的厉害那帮暴力驾驶的估计...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9920</th>\n",
       "      <td>4</td>\n",
       "      <td>看路况，省道多平均油耗8-9，市区多11-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9921</th>\n",
       "      <td>5</td>\n",
       "      <td>换挡不是根据速度，而是根据你的意图。你想省油并匀速驾驶，就在转速允许的情况档位越高越好。想要...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9922</th>\n",
       "      <td>7</td>\n",
       "      <td>换挡不是根据速度，而是根据你的意图。你想省油并匀速驾驶，就在转速允许的情况档位越高越好。想要...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9923</th>\n",
       "      <td>1</td>\n",
       "      <td>同款配置黑森还要下个月中旬才能提车</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9924</th>\n",
       "      <td>4</td>\n",
       "      <td>正常，我城里跑油耗11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9925</th>\n",
       "      <td>1</td>\n",
       "      <td>森林人做工一般，配置还不如8、9万的国产车，这是事实。在老家侄儿子的五菱宏光把我的森林人给甩...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9926</th>\n",
       "      <td>6</td>\n",
       "      <td>森林人做工一般，配置还不如8、9万的国产车，这是事实。在老家侄儿子的五菱宏光把我的森林人给甩...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9927</th>\n",
       "      <td>1</td>\n",
       "      <td>导航已经换成凯立德了，其它的功能也很差，我只是用来听收音机，导航还是手机的好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9928</th>\n",
       "      <td>1</td>\n",
       "      <td>有没有换过导航啥的，一般新车很少电路故障，行车电脑问题几率大点！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9929</th>\n",
       "      <td>4</td>\n",
       "      <td>感觉稍微有点高了，油耗高低和很多方面有关，比如你个人驾驶习惯，你行驶道路拥堵状况等等，平稳驾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9930</th>\n",
       "      <td>5</td>\n",
       "      <td>选12款之前的。12款后都是fb发动机， 机油消耗大。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9931</th>\n",
       "      <td>7</td>\n",
       "      <td>跟欧蓝德的调校很相似啊，掌握规律后，堵车时跟车，可减少踩刹车的次数</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9932</th>\n",
       "      <td>2</td>\n",
       "      <td>哈哈哈，欧兰德也能上来跟森比，那破车你开几年试试，那操控也能比</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9933</th>\n",
       "      <td>4</td>\n",
       "      <td>14款2.0，大连，七万多公里了，平均油耗10。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9934</th>\n",
       "      <td>4</td>\n",
       "      <td>森林人2.0开了两年，一直在中石化加油站加92#，没啥问题，杠杠的，夏天开空调油耗8.5左右。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9935</th>\n",
       "      <td>3</td>\n",
       "      <td>森林人2.0开了两年，一直在中石化加油站加92#，没啥问题，杠杠的，夏天开空调油耗8.5左右。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9936</th>\n",
       "      <td>0</td>\n",
       "      <td>国内轮胎的品牌还是比较多的，看你要准备用什么价位的了，一般来说声音越小的价格也就高些。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9937</th>\n",
       "      <td>7</td>\n",
       "      <td>我的也这样，换刹车片后消失</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9938</th>\n",
       "      <td>5</td>\n",
       "      <td>反正我是不会去了，除非关健部位如变速箱油</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9939</th>\n",
       "      <td>4</td>\n",
       "      <td>我感觉你的油耗稍大些，我这里山东，最冷零下18度，最高6度或者0度左右，也是16款2.0的，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9940</th>\n",
       "      <td>2</td>\n",
       "      <td>我的方向盘也是两边缝不一样大</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>4</td>\n",
       "      <td>俺必须的说，15公里以上的上下班单程行程，很少红灯，80左右的速度……对小森是极好的……所以...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>9</td>\n",
       "      <td>老铁没毛病...双击666！！！是不是这样？我这理解逻辑也挑不出毛病吧！你不矫情你试试？多大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9943</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0时尚，四川这段时间温度最低-1，基本上下班用，现在均速18，表显油耗9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9944</th>\n",
       "      <td>6</td>\n",
       "      <td>相貌不起眼，内饰太平淡，开起来才知道好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9945</th>\n",
       "      <td>1</td>\n",
       "      <td>一看就知道山寨的！用的肯定是中控的普通视频输入接口而已，这种无屏的记录仪淘宝也就一两百块的东...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9946</th>\n",
       "      <td>5</td>\n",
       "      <td>楼主还在吗，同在上海，想咨询一下家用首辆车，到底是买森林人还是力狮好？如果森林人，2.0的动...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9947 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sub                                            content\n",
       "0       0           因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。\n",
       "1       0      四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。\n",
       "2       0  斯柯达要说质量，似乎比大众要好一点，价格也低一些，用料完全一样。我听说过野帝，但没听说过你说...\n",
       "3       0           这玩意都是给有钱任性又不懂车的土豪用的，这价格换一次我妹夫EP020可以换三锅了\n",
       "4       0                            17价格忒高，估计也就是14-15左右。   \n",
       "5       0  我开始就是荣放2.5  森林人2.5二选一    荣放主要是底盘质感不行   太硬  其次是...\n",
       "6       0                   唉，这货的价格死硬死硬的，低配版优惠1万据说已经罕有了。    \n",
       "7       0                      价格的话只能说一般般吧，太仓前段时间定的比你便宜！    \n",
       "8       0                                听过，价格太贵，但一直念念不忘    \n",
       "9       0                                      恭喜恭喜，这个优惠不错哦！\n",
       "10      0                                     优惠幅度不小了，北京优惠八千\n",
       "11      0                         优惠可以了！购进吧！买了不会后悔的！时间可鉴！   \n",
       "12      0                                        现在 什么价 优惠多少\n",
       "13      0                                 优惠一万一 送贴膜装甲脚垫 铁西庞大\n",
       "14      0                             我也大连的，最近也考虑入手森林人，优惠太小了\n",
       "15      0                                  山东威海全系这才优惠3000，MD\n",
       "16      0                                    下手了，豪导特供，优惠1.6万\n",
       "17      0                                 优惠了8000，什么都不送。    \n",
       "18      0                          兄弟2.5豪导特供优惠1.6万可以下手吗？    \n",
       "19      0                          恭喜，优惠多少，我准备明天去提，2.5特装优惠两万\n",
       "20      0  什么车款，多少优惠啊？康桥我只能谈到优惠1.5其他还送什么没问，试驾了一次看了两次2.0时尚...\n",
       "21      0                                   优惠1 ，就送行程记录     \n",
       "22      0  平常心吧，我本来想买傲虎，优惠不大，钱不够，改提森林人，6月提尊贵优惠1.4w，到了9月做活...\n",
       "23      0                                 哈哈，25.48。优惠够给力    \n",
       "24      0  兄弟，我也云南的，你的优惠也挺大的，现在哪里来的全系3万啊。。。现傲虎全系无优惠，森林人才八...\n",
       "25      0                  来湖北，我上月订的2.0蓝色时尚，优惠1万3送6次保养，本月底提车\n",
       "26      0                            什么都没有，就是现金优惠和给了点小东西    \n",
       "27      0                什么都没有，就是现金优惠和给了点小东西，你最好先问问看有没有车    \n",
       "28      0                             原来最低也就优惠15000至16000   \n",
       "29      0                                   我们这里，优惠2000，我呵呵了\n",
       "...   ...                                                ...\n",
       "9917    3                                  风噪大 音响差驾驶什么的，秒杀其他\n",
       "9918    7               安全大于一切，出远门前一定要检查车子，主要有全车油水，轮胎，刹车系统等。\n",
       "9919    5  发动机直接就不一样了 FA20是斯巴鲁的招牌发动机 给WRX用要是烧的厉害那帮暴力驾驶的估计...\n",
       "9920    4                            看路况，省道多平均油耗8-9，市区多11-12\n",
       "9921    5  换挡不是根据速度，而是根据你的意图。你想省油并匀速驾驶，就在转速允许的情况档位越高越好。想要...\n",
       "9922    7  换挡不是根据速度，而是根据你的意图。你想省油并匀速驾驶，就在转速允许的情况档位越高越好。想要...\n",
       "9923    1                                  同款配置黑森还要下个月中旬才能提车\n",
       "9924    4                                        正常，我城里跑油耗11\n",
       "9925    1  森林人做工一般，配置还不如8、9万的国产车，这是事实。在老家侄儿子的五菱宏光把我的森林人给甩...\n",
       "9926    6  森林人做工一般，配置还不如8、9万的国产车，这是事实。在老家侄儿子的五菱宏光把我的森林人给甩...\n",
       "9927    1             导航已经换成凯立德了，其它的功能也很差，我只是用来听收音机，导航还是手机的好\n",
       "9928    1                   有没有换过导航啥的，一般新车很少电路故障，行车电脑问题几率大点！\n",
       "9929    4  感觉稍微有点高了，油耗高低和很多方面有关，比如你个人驾驶习惯，你行驶道路拥堵状况等等，平稳驾...\n",
       "9930    5                        选12款之前的。12款后都是fb发动机， 机油消耗大。\n",
       "9931    7                  跟欧蓝德的调校很相似啊，掌握规律后，堵车时跟车，可减少踩刹车的次数\n",
       "9932    2                    哈哈哈，欧兰德也能上来跟森比，那破车你开几年试试，那操控也能比\n",
       "9933    4                           14款2.0，大连，七万多公里了，平均油耗10。\n",
       "9934    4    森林人2.0开了两年，一直在中石化加油站加92#，没啥问题，杠杠的，夏天开空调油耗8.5左右。\n",
       "9935    3    森林人2.0开了两年，一直在中石化加油站加92#，没啥问题，杠杠的，夏天开空调油耗8.5左右。\n",
       "9936    0        国内轮胎的品牌还是比较多的，看你要准备用什么价位的了，一般来说声音越小的价格也就高些。\n",
       "9937    7                                      我的也这样，换刹车片后消失\n",
       "9938    5                               反正我是不会去了，除非关健部位如变速箱油\n",
       "9939    4  我感觉你的油耗稍大些，我这里山东，最冷零下18度，最高6度或者0度左右，也是16款2.0的，...\n",
       "9940    2                                     我的方向盘也是两边缝不一样大\n",
       "9941    4  俺必须的说，15公里以上的上下班单程行程，很少红灯，80左右的速度……对小森是极好的……所以...\n",
       "9942    9  老铁没毛病...双击666！！！是不是这样？我这理解逻辑也挑不出毛病吧！你不矫情你试试？多大...\n",
       "9943    4           2.0时尚，四川这段时间温度最低-1，基本上下班用，现在均速18，表显油耗9.2\n",
       "9944    6                                相貌不起眼，内饰太平淡，开起来才知道好\n",
       "9945    1  一看就知道山寨的！用的肯定是中控的普通视频输入接口而已，这种无屏的记录仪淘宝也就一两百块的东...\n",
       "9946    5  楼主还在吗，同在上海，想咨询一下家用首辆车，到底是买森林人还是力狮好？如果森林人，2.0的动...\n",
       "\n",
       "[9947 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vecs = []\n",
    "for i in range(len(sentences)):\n",
    "    temp = []\n",
    "    for j in range(len(sentences[i])):\n",
    "        temp.append(model[sentences[i][j]])\n",
    "    while len(temp) < 50:\n",
    "        temp.append([0.0] * 50)\n",
    "    if len(temp) > 50:\n",
    "        temp = temp[:50]\n",
    "    x_vecs.append(temp)\n",
    "x_vecs = np.array(x_vecs)\n",
    "print(x_vecs.shape)\n",
    "y_map = []\n",
    "map_ = ['价格', '配置', '操控', '舒适性', '油耗', '动力', '内饰', '安全性', '空间', '外观']\n",
    "y = list(data['subject'])\n",
    "for i in range(len(y)):\n",
    "    y_map.append(map_.index(y[i]))\n",
    "y_map = np.array(y_map)\n",
    "y_map.shape\n",
    "new_df = {'sub':y_map, 'content':list(data['content'])}\n",
    "new_df = pd.DataFrame(new_df)\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_vecs\n",
    "y = y_map\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6664, 6664, 3283)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x), len(train_y), len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [(train_x[i], train_y[i]) for i in range(len(train_x))]\n",
    "testset = [(test_x[i], test_y[i]) for i in range(len(test_x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = t.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=4,\n",
    "                    shuffle=True, \n",
    "                    num_workers=2)\n",
    "testloader = t.utils.data.DataLoader(\n",
    "                    testset, \n",
    "                    batch_size=4,\n",
    "                    shuffle=False, \n",
    "                    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (rnn1): RNN(50, 50)\n",
      "  (fc1): Linear(in_features=625, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn1 = nn.RNN(50, 50) \n",
    "        self.fc1   = nn.Linear(25 * 25, 120) \n",
    "        self.fc2   = nn.Linear(120, 50)\n",
    "        self.fc3   = nn.Linear(50, 10)\n",
    "    def forward(self, x): \n",
    "        output, hn =  self.rnn1(x.float())\n",
    "        x = F.max_pool2d(F.relu(output), (2, 2))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)         \n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.221\n",
      "[1,   400] loss: 0.213\n",
      "[1,   600] loss: 0.214\n",
      "[1,   800] loss: 0.212\n",
      "[1,  1000] loss: 0.210\n",
      "[1,  1200] loss: 0.216\n",
      "[1,  1400] loss: 0.214\n",
      "[1,  1600] loss: 0.215\n",
      "[2,   200] loss: 0.212\n",
      "[2,   400] loss: 0.211\n",
      "[2,   600] loss: 0.209\n",
      "[2,   800] loss: 0.208\n",
      "[2,  1000] loss: 0.208\n",
      "[2,  1200] loss: 0.201\n",
      "[2,  1400] loss: 0.203\n",
      "[2,  1600] loss: 0.204\n",
      "[3,   200] loss: 0.203\n",
      "[3,   400] loss: 0.198\n",
      "[3,   600] loss: 0.198\n",
      "[3,   800] loss: 0.195\n",
      "[3,  1000] loss: 0.198\n",
      "[3,  1200] loss: 0.192\n",
      "[3,  1400] loss: 0.189\n",
      "[3,  1600] loss: 0.190\n",
      "[4,   200] loss: 0.189\n",
      "[4,   400] loss: 0.180\n",
      "[4,   600] loss: 0.181\n",
      "[4,   800] loss: 0.184\n",
      "[4,  1000] loss: 0.188\n",
      "[4,  1200] loss: 0.185\n",
      "[4,  1400] loss: 0.185\n",
      "[4,  1600] loss: 0.182\n",
      "[5,   200] loss: 0.175\n",
      "[5,   400] loss: 0.184\n",
      "[5,   600] loss: 0.171\n",
      "[5,   800] loss: 0.174\n",
      "[5,  1000] loss: 0.177\n",
      "[5,  1200] loss: 0.176\n",
      "[5,  1400] loss: 0.167\n",
      "[5,  1600] loss: 0.169\n",
      "[6,   200] loss: 0.168\n",
      "[6,   400] loss: 0.159\n",
      "[6,   600] loss: 0.166\n",
      "[6,   800] loss: 0.176\n",
      "[6,  1000] loss: 0.170\n",
      "[6,  1200] loss: 0.166\n",
      "[6,  1400] loss: 0.174\n",
      "[6,  1600] loss: 0.157\n",
      "[7,   200] loss: 0.165\n",
      "[7,   400] loss: 0.163\n",
      "[7,   600] loss: 0.161\n",
      "[7,   800] loss: 0.158\n",
      "[7,  1000] loss: 0.166\n",
      "[7,  1200] loss: 0.156\n",
      "[7,  1400] loss: 0.161\n",
      "[7,  1600] loss: 0.160\n",
      "[8,   200] loss: 0.157\n",
      "[8,   400] loss: 0.153\n",
      "[8,   600] loss: 0.158\n",
      "[8,   800] loss: 0.157\n",
      "[8,  1000] loss: 0.153\n",
      "[8,  1200] loss: 0.153\n",
      "[8,  1400] loss: 0.153\n",
      "[8,  1600] loss: 0.151\n",
      "[9,   200] loss: 0.153\n",
      "[9,   400] loss: 0.150\n",
      "[9,   600] loss: 0.146\n",
      "[9,   800] loss: 0.150\n",
      "[9,  1000] loss: 0.148\n",
      "[9,  1200] loss: 0.139\n",
      "[9,  1400] loss: 0.150\n",
      "[9,  1600] loss: 0.152\n",
      "[10,   200] loss: 0.142\n",
      "[10,   400] loss: 0.138\n",
      "[10,   600] loss: 0.143\n",
      "[10,   800] loss: 0.141\n",
      "[10,  1000] loss: 0.142\n",
      "[10,  1200] loss: 0.152\n",
      "[10,  1400] loss: 0.146\n",
      "[10,  1600] loss: 0.139\n",
      "[11,   200] loss: 0.141\n",
      "[11,   400] loss: 0.129\n",
      "[11,   600] loss: 0.141\n",
      "[11,   800] loss: 0.138\n",
      "[11,  1000] loss: 0.144\n",
      "[11,  1200] loss: 0.139\n",
      "[11,  1400] loss: 0.137\n",
      "[11,  1600] loss: 0.146\n",
      "[12,   200] loss: 0.128\n",
      "[12,   400] loss: 0.132\n",
      "[12,   600] loss: 0.135\n",
      "[12,   800] loss: 0.130\n",
      "[12,  1000] loss: 0.136\n",
      "[12,  1200] loss: 0.138\n",
      "[12,  1400] loss: 0.140\n",
      "[12,  1600] loss: 0.139\n",
      "[13,   200] loss: 0.124\n",
      "[13,   400] loss: 0.132\n",
      "[13,   600] loss: 0.131\n",
      "[13,   800] loss: 0.134\n",
      "[13,  1000] loss: 0.128\n",
      "[13,  1200] loss: 0.136\n",
      "[13,  1400] loss: 0.137\n",
      "[13,  1600] loss: 0.133\n",
      "[14,   200] loss: 0.123\n",
      "[14,   400] loss: 0.127\n",
      "[14,   600] loss: 0.131\n",
      "[14,   800] loss: 0.133\n",
      "[14,  1000] loss: 0.133\n",
      "[14,  1200] loss: 0.130\n",
      "[14,  1400] loss: 0.122\n",
      "[14,  1600] loss: 0.132\n",
      "[15,   200] loss: 0.122\n",
      "[15,   400] loss: 0.123\n",
      "[15,   600] loss: 0.128\n",
      "[15,   800] loss: 0.122\n",
      "[15,  1000] loss: 0.129\n",
      "[15,  1200] loss: 0.126\n",
      "[15,  1400] loss: 0.131\n",
      "[15,  1600] loss: 0.122\n",
      "[16,   200] loss: 0.121\n",
      "[16,   400] loss: 0.120\n",
      "[16,   600] loss: 0.118\n",
      "[16,   800] loss: 0.119\n",
      "[16,  1000] loss: 0.119\n",
      "[16,  1200] loss: 0.131\n",
      "[16,  1400] loss: 0.124\n",
      "[16,  1600] loss: 0.127\n",
      "[17,   200] loss: 0.120\n",
      "[17,   400] loss: 0.115\n",
      "[17,   600] loss: 0.117\n",
      "[17,   800] loss: 0.122\n",
      "[17,  1000] loss: 0.118\n",
      "[17,  1200] loss: 0.121\n",
      "[17,  1400] loss: 0.119\n",
      "[17,  1600] loss: 0.125\n",
      "[18,   200] loss: 0.108\n",
      "[18,   400] loss: 0.114\n",
      "[18,   600] loss: 0.114\n",
      "[18,   800] loss: 0.117\n",
      "[18,  1000] loss: 0.119\n",
      "[18,  1200] loss: 0.116\n",
      "[18,  1400] loss: 0.125\n",
      "[18,  1600] loss: 0.118\n",
      "[19,   200] loss: 0.109\n",
      "[19,   400] loss: 0.106\n",
      "[19,   600] loss: 0.114\n",
      "[19,   800] loss: 0.110\n",
      "[19,  1000] loss: 0.118\n",
      "[19,  1200] loss: 0.120\n",
      "[19,  1400] loss: 0.113\n",
      "[19,  1600] loss: 0.113\n",
      "[20,   200] loss: 0.108\n",
      "[20,   400] loss: 0.112\n",
      "[20,   600] loss: 0.103\n",
      "[20,   800] loss: 0.106\n",
      "[20,  1000] loss: 0.115\n",
      "[20,  1200] loss: 0.110\n",
      "[20,  1400] loss: 0.111\n",
      "[20,  1600] loss: 0.116\n",
      "[21,   200] loss: 0.102\n",
      "[21,   400] loss: 0.108\n",
      "[21,   600] loss: 0.105\n",
      "[21,   800] loss: 0.110\n",
      "[21,  1000] loss: 0.101\n",
      "[21,  1200] loss: 0.111\n",
      "[21,  1400] loss: 0.113\n",
      "[21,  1600] loss: 0.113\n",
      "[22,   200] loss: 0.096\n",
      "[22,   400] loss: 0.106\n",
      "[22,   600] loss: 0.103\n",
      "[22,   800] loss: 0.103\n",
      "[22,  1000] loss: 0.102\n",
      "[22,  1200] loss: 0.112\n",
      "[22,  1400] loss: 0.107\n",
      "[22,  1600] loss: 0.108\n",
      "[23,   200] loss: 0.093\n",
      "[23,   400] loss: 0.097\n",
      "[23,   600] loss: 0.100\n",
      "[23,   800] loss: 0.100\n",
      "[23,  1000] loss: 0.102\n",
      "[23,  1200] loss: 0.105\n",
      "[23,  1400] loss: 0.106\n",
      "[23,  1600] loss: 0.102\n",
      "[24,   200] loss: 0.086\n",
      "[24,   400] loss: 0.092\n",
      "[24,   600] loss: 0.093\n",
      "[24,   800] loss: 0.102\n",
      "[41,  1400] loss: 0.058\n",
      "[41,  1600] loss: 0.062\n",
      "[42,   200] loss: 0.050\n",
      "[42,   400] loss: 0.046\n",
      "[42,   600] loss: 0.059\n",
      "[42,   800] loss: 0.055\n",
      "[42,  1000] loss: 0.054\n",
      "[42,  1200] loss: 0.057\n",
      "[42,  1400] loss: 0.064\n",
      "[42,  1600] loss: 0.056\n",
      "[43,   200] loss: 0.048\n",
      "[43,   400] loss: 0.048\n",
      "[43,   600] loss: 0.052\n",
      "[43,   800] loss: 0.058\n",
      "[43,  1000] loss: 0.052\n",
      "[43,  1200] loss: 0.056\n",
      "[43,  1400] loss: 0.062\n",
      "[43,  1600] loss: 0.060\n",
      "[44,   200] loss: 0.046\n",
      "[44,   400] loss: 0.048\n",
      "[44,   600] loss: 0.046\n",
      "[44,   800] loss: 0.051\n",
      "[44,  1000] loss: 0.054\n",
      "[44,  1200] loss: 0.059\n",
      "[44,  1400] loss: 0.054\n",
      "[44,  1600] loss: 0.063\n",
      "[45,   200] loss: 0.045\n",
      "[45,   400] loss: 0.048\n",
      "[45,   600] loss: 0.051\n",
      "[45,   800] loss: 0.052\n",
      "[45,  1000] loss: 0.056\n",
      "[45,  1200] loss: 0.052\n",
      "[45,  1400] loss: 0.055\n",
      "[45,  1600] loss: 0.058\n",
      "[46,   200] loss: 0.040\n",
      "[46,   400] loss: 0.047\n",
      "[46,   600] loss: 0.046\n",
      "[46,   800] loss: 0.050\n",
      "[46,  1000] loss: 0.052\n",
      "[46,  1200] loss: 0.056\n",
      "[46,  1400] loss: 0.055\n",
      "[46,  1600] loss: 0.055\n",
      "[47,   200] loss: 0.040\n",
      "[47,   400] loss: 0.044\n",
      "[47,   600] loss: 0.051\n",
      "[47,   800] loss: 0.045\n",
      "[47,  1000] loss: 0.050\n",
      "[47,  1200] loss: 0.049\n",
      "[47,  1400] loss: 0.056\n",
      "[47,  1600] loss: 0.058\n",
      "[48,   200] loss: 0.042\n",
      "[48,   400] loss: 0.044\n",
      "[48,   600] loss: 0.048\n",
      "[48,   800] loss: 0.046\n",
      "[48,  1000] loss: 0.049\n",
      "[48,  1200] loss: 0.056\n",
      "[48,  1400] loss: 0.062\n",
      "[48,  1600] loss: 0.054\n",
      "[49,   200] loss: 0.038\n",
      "[49,   400] loss: 0.044\n",
      "[49,   600] loss: 0.048\n",
      "[49,   800] loss: 0.041\n",
      "[49,  1000] loss: 0.046\n",
      "[49,  1200] loss: 0.051\n",
      "[49,  1400] loss: 0.051\n",
      "[49,  1600] loss: 0.058\n",
      "[50,   200] loss: 0.040\n",
      "[50,   400] loss: 0.039\n",
      "[50,   600] loss: 0.047\n",
      "[50,   800] loss: 0.047\n",
      "[50,  1000] loss: 0.046\n",
      "[50,  1200] loss: 0.048\n",
      "[50,  1400] loss: 0.058\n",
      "[50,  1600] loss: 0.049\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "t.set_num_threads(8)\n",
    "for epoch in range(50):  \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # 输入数据\n",
    "        inputs, labels = data\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()   \n",
    "        \n",
    "        # 更新参数 \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印log信息\n",
    "        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199: # 每2000个batch打印一下训练状态\n",
    "            print('[%d, %5d] loss: %.3f' \\\n",
    "                  % (epoch+1, i+1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实际的label:        油耗       动力       动力       操控\n",
      "预测结果:     油耗    动力    内饰    操控\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next() # 一个batch返回4张图片\n",
    "print('实际的label: ', ' '.join(\\\n",
    "            '%08s'%map_[labels[j]] for j in range(4)))\n",
    "\n",
    "# 计算图片在每个类别上的分数\n",
    "outputs = net(images)\n",
    "# 得分最高的那个类\n",
    "_, predicted = t.max(outputs.data, 1)\n",
    "\n",
    "print('预测结果: ', ' '.join('%5s'\\\n",
    "            % map_[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283张测试集中的准确率为: 48 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0 # 预测正确的图片数\n",
    "total = 0 # 总共的图片数\n",
    "\n",
    "\n",
    "# 由于测试的时候不需要求导，可以暂时关闭autograd，提高速度，节约内存\n",
    "with t.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = t.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "print('3283张测试集中的准确率为: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yxvenv]",
   "language": "python",
   "name": "conda-env-yxvenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
