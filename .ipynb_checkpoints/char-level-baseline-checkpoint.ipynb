{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision as tv\n",
    "import jieba\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch\n",
    "show = ToPILImage() # 可以把Tensor转成Image，方便可视化\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata = pd.read_csv('../../data/df_sen_sub/train.csv')\\nlen(data['content_id'].unique()), len(list(data['content_id']))\\ncontent_ids = data['content_id'].unique()\\nsubjs = data['subject'].unique()\\ndf = {}\\ntext, subs, sentis = [], [], []\\nfor i in range(len(content_ids)):\\n    if i % 100 == 0:print(i)\\n    dd = data[data['content_id'] == content_ids[i]]\\n    text.append(dd.loc[dd.index.tolist()[0], 'content'])\\n    temp_subs, temp_senti = [], []\\n    for sub in subjs:\\n        temp_subs.append(sub)\\n        if len(dd[dd['subject'] == sub]) != 0:\\n            temp_senti.append(str(int(dd[dd['subject'] == sub]['sentiment_value'])))\\n        else:\\n            temp_senti.append('2')\\n    subs.append(','.join(temp_subs))\\n    sentis.append(','.join(temp_senti))\\ndf['content'] = text\\ndf['subjects'] = subs\\ndf['sentiments'] = sentis\\ndf = pd.DataFrame(df)\\ndf.to_csv('../../data/df_sen_sub/merge_train.csv', index = False)\\n#data = pd.read_csv('../../data/df_sen_sub/test_public.csv')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data = pd.read_csv('../../data/df_sen_sub/train.csv')\n",
    "len(data['content_id'].unique()), len(list(data['content_id']))\n",
    "content_ids = data['content_id'].unique()\n",
    "subjs = data['subject'].unique()\n",
    "df = {}\n",
    "text, subs, sentis = [], [], []\n",
    "for i in range(len(content_ids)):\n",
    "    if i % 100 == 0:print(i)\n",
    "    dd = data[data['content_id'] == content_ids[i]]\n",
    "    text.append(dd.loc[dd.index.tolist()[0], 'content'])\n",
    "    temp_subs, temp_senti = [], []\n",
    "    for sub in subjs:\n",
    "        temp_subs.append(sub)\n",
    "        if len(dd[dd['subject'] == sub]) != 0:\n",
    "            temp_senti.append(str(int(dd[dd['subject'] == sub]['sentiment_value'])))\n",
    "        else:\n",
    "            temp_senti.append('2')\n",
    "    subs.append(','.join(temp_subs))\n",
    "    sentis.append(','.join(temp_senti))\n",
    "df['content'] = text\n",
    "df['subjects'] = subs\n",
    "df['sentiments'] = sentis\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv('../../data/df_sen_sub/merge_train.csv', index = False)\n",
    "#data = pd.read_csv('../../data/df_sen_sub/test_public.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/df_sen_sub/merge_train.csv')\n",
    "data = data.fillna('')\n",
    "subs = list(data['subjects'])\n",
    "sentiments = list(data['sentiments'])\n",
    "content = list(data['content'])\n",
    "test_data = pd.read_csv('../../data/df_sen_sub/test_public.csv')\n",
    "test_data = test_data.fillna('')\n",
    "test_content = list(test_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/df_sen_sub/char_add_text.txt','w') as f:\n",
    "    f.write('\\n'.join(list(content) + list(test_content)  + list(subs)))\n",
    "# 迭代器，使用jieba将句子进行分词\n",
    "class Sentences(object):# 这个类可以根据实际情况重写，我已经将所有的文章进行分句，并整合到了一个文件里面\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname # 句子所在文件，没句句子占一行\n",
    "        #jieba.load_userdict(\"wordBase.txt\") # 加载词库\n",
    "\n",
    "    def __iter__(self):\n",
    "        #for fname in os.listdir(self.dirname):\n",
    "        for line in open(self.dirname):\n",
    "                line = line.replace('\\n', '')\n",
    "                if line == '价格,配置,操控,舒适性,油耗,动力,内饰,安全性,空间,外观':\n",
    "                    yield line.split(',')\n",
    "                else:\n",
    "                    yield list(line)\n",
    "\n",
    "sentences = []\n",
    "def train_word2vec(folder_path, size=100):\n",
    "    global sentences\n",
    "    sentences = Sentences(folder_path) #生成分词后的句子，是一个二维数组\n",
    "\n",
    "    # size是词向量长度\n",
    "    # worker是线程数量，建议与物理线程数量一致\n",
    "    # min_count是指出现次数小于一定程度，就忽略，0表示不忽略\n",
    "    model = Word2Vec(sentences, size=size, workers=8, min_count=0)\n",
    "\n",
    "    # 训练结束就将模型保存起来\n",
    "    model.save(\"../../data/df_sen_sub/char_add_word2vec_model\")\n",
    "\n",
    "# 生成50维度的词向量模型\n",
    "train_word2vec(\"../../data/df_sen_sub/char_add_text.txt\",100)\n",
    "\n",
    "# 测试训练好的词向量模型，使用model[keyWord]即可获取keyword这个词的词向量\n",
    "model = Word2Vec.load(\"../../data/df_sen_sub/char_add_word2vec_model\")\n",
    "sentences = list(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8290, 50, 100) (2364, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "x_vecs = []\n",
    "x_test_vecs = []  \n",
    "ind = 0\n",
    "word2ind = {}\n",
    "pretrained = []\n",
    "senti_sen = []\n",
    "senti_test_sen = []\n",
    "for i in range(len(sentences) - len(content) + 1):\n",
    "    if i < len(content):\n",
    "        temp = []\n",
    "        t_s = []\n",
    "        for j in range(len(sentences[i])):\n",
    "            if sentences[i][j] not in word2ind:\n",
    "                pretrained.append(model[sentences[i][j]])\n",
    "                word2ind[sentences[i][j]] = ind\n",
    "                ind += 1\n",
    "            t_s.append(sentences[i][j])\n",
    "            temp.append(model[sentences[i][j]])\n",
    "        while len(temp) < 50:\n",
    "            temp.append([0.0] * 100)\n",
    "        if len(temp) > 50:\n",
    "            temp = temp[:50]\n",
    "        x_vecs.append(temp)\n",
    "        senti_sen.append(t_s[:50])\n",
    "    elif i != len(sentences) - len(content):\n",
    "        temp = []\n",
    "        t_s = []\n",
    "        for j in range(len(sentences[i])):\n",
    "            if sentences[i][j] not in word2ind:\n",
    "                pretrained.append(model[sentences[i][j]])\n",
    "                word2ind[sentences[i][j]] = ind\n",
    "                ind += 1\n",
    "            t_s.append(sentences[i][j])\n",
    "            temp.append(model[sentences[i][j]])\n",
    "        while len(temp) < 50:\n",
    "            temp.append([0.0] * 100)\n",
    "        if len(temp) > 50:\n",
    "            temp = temp[:50]\n",
    "        x_test_vecs.append(temp)\n",
    "        senti_test_sen.append(t_s[:50])  \n",
    "    else:\n",
    "        for j in range(len(sentences[i])):\n",
    "            if sentences[i][j] not in word2ind:\n",
    "                pretrained.append(model[sentences[i][j]])\n",
    "                word2ind[sentences[i][j]] = ind\n",
    "                ind += 1        \n",
    "x_vecs = np.array(x_vecs)\n",
    "x_test_vecs = np.array(x_test_vecs)\n",
    "print(x_vecs.shape, x_test_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2875, 2865)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2ind),word2ind['价格']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#va, content_vecs, y\n",
    "pretrained.append([0.0] * 100)\n",
    "word2ind['null'] = 2875\n",
    "map_ = ['-1', '0', '1', '2']\n",
    "sub_set = {}\n",
    "for i in range(len(subs)):\n",
    "    subjs = subs[i].split(',')\n",
    "    sentis = sentiments[i].split(',')\n",
    "    for j in range(len(subjs)):\n",
    "        if subjs[j] not in sub_set:\n",
    "            sub_set[subjs[j]] = []\n",
    "        sub_set[subjs[j]].append(map_.index(sentis[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = np.array(pretrained)\n",
    "han_x = []\n",
    "for i in range(len(senti_sen)):\n",
    "    temp = []\n",
    "    for j in range(len(senti_sen[i])):\n",
    "        temp.append(word2ind[senti_sen[i][j]])\n",
    "    while len(temp) < 50:\n",
    "        temp.append(2875)\n",
    "\n",
    "    temp = temp[:50]\n",
    "\n",
    "    han_x.append(temp)\n",
    "han_x = np.array(han_x)\n",
    "\n",
    "han_x_test = []\n",
    "for i in range(len(senti_test_sen)):\n",
    "    temp = []\n",
    "    for j in range(len(senti_test_sen[i])):\n",
    "        temp.append(word2ind[senti_test_sen[i][j]])\n",
    "    while len(temp) < 50:\n",
    "        temp.append(2875)\n",
    "\n",
    "    temp = temp[:50]\n",
    "\n",
    "    han_x_test.append(temp)\n",
    "han_x = np.array(han_x)\n",
    "han_x_test = np.array(han_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pretrained, vocab_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size = 100, hidden_size = 100, bidirectional = True) \n",
    "        #self.fc1   = nn.Linear(3750, 50) \n",
    "        #self.fc2   = nn.Linear(200, 50)\n",
    "        #self.fc3   = nn.Linear(200, 50)\n",
    "        #self.fc4   = nn.Linear(100, 3)\n",
    "        self.fctest   = nn.Linear(200, 3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, 100)\n",
    "        pretrained_weight = np.array(pretrained)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        \n",
    "    def forward(self, x, aspects): \n",
    "        x = self.embeddings(x)#batch,50,100\n",
    "        #va = self.embeddings(aspects[0])\n",
    "        #t_va = va.unsqueeze(0).expand(x.shape[0], x.shape[1], 100)\n",
    "        output, (hn, cn) =  self.bilstm(x.float())#batch,50,200\n",
    "        H, hN = output, output[:,-1,:]\n",
    "        #con = torch.cat((t_va, H), 2)\n",
    "        #t1 = F.max_pool2d(F.relu(con), (1, 4))\n",
    "        #con = t1.view(x.size()[0], -1)\n",
    "        #con = F.relu(self.fc1(con))\n",
    "        #alpha = con.unsqueeze(1)\n",
    "        #rr = torch.matmul(alpha,H).squeeze(1)\n",
    "        #yy = torch.cat((F.relu(self.fc2(rr)), F.relu(self.fc3(hN))), 1)\n",
    "        #x = self.fc4(yy)\n",
    "        x  = self.fctest(hN)\n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net(pretrained, len(pretrained))\n",
    "print(net)\n",
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "'''\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pretrained, vocab_size):\n",
    "        super(Net, self).__init__()\n",
    "        #self.rnn1 = nn.RNN(100, 100) \n",
    "        self.bilstm = nn.LSTM(input_size = 100, hidden_size = 100, bidirectional = True)\n",
    "        #self.fc1   = nn.Linear(1250, 120) \n",
    "        self.fch = nn.Linear(200, 100)\n",
    "        self.fcv = nn.Linear(100, 100)\n",
    "        self.fcw = nn.Linear(200, 1)\n",
    "        self.fcwp = nn.Linear(200,50)\n",
    "        self.fcwh = nn.Linear(200,50)\n",
    "        self.fc1   = nn.Linear(3750, 120) \n",
    "        self.fc2   = nn.Linear(120, 50)\n",
    "        self.fc3   = nn.Linear(50, 4)\n",
    "        self.embeddings = nn.Embedding(vocab_size, 100)\n",
    "        pretrained_weight = np.array(pretrained)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        \n",
    "    def forward(self, x, aspects): \n",
    "        x = self.embeddings(x) #[4,50,100]\n",
    "        batch_size = x.shape[0]\n",
    "        N = x.shape[1]\n",
    "        output, (hn, cn) =  self.bilstm(x.float())  #[4,50,200]\n",
    "        hN = output[:,-1,:]\n",
    "        va = self.embeddings(aspects[0]) #[1, 100]\n",
    "        t_va = torch.matmul(torch.ones((x.shape[0],50, 1)), va)#[4, 50, 100]\n",
    "        t_va = t_va.reshape((-1, 100)) #[4*50, 100]\n",
    "        x = output.reshape((-1, 200)) #[4*50, 200]\n",
    "        x = torch.tanh(torch.cat((self.fch(x), self.fcv(t_va)), 1))\n",
    "        M = x.reshape((batch_size, N, -1)) #[4, 50, 200]\n",
    "        alpha = F.softmax(self.fcw(M)).reshape((batch_size, 1, -1))#[4,50]\n",
    "        rr = torch.matmul(alpha, output).reshape((batch_size, -1)) #[4, 200]\n",
    "        final = torch.tanh(self.fcwh(hN) + self.fcwp(rr))\n",
    "        x = self.fc3(final)   \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (bilstm): LSTM(100, 100, bidirectional=True)\n",
      "  (fch): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fcv): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fcw): Linear(in_features=200, out_features=1, bias=True)\n",
      "  (fcwp): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fcwh): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc1): Linear(in_features=3750, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (embeddings): Embedding(2866, 100)\n",
      ")\n",
      "> \u001b[0;32m<ipython-input-10-97abb248ab01>\u001b[0m(38)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     37 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 38 \u001b[0;31m            \u001b[0mva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     39 \u001b[0;31m            \u001b[0mva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> key\n",
      "'价格'\n",
      "ipdb> word2ind['价']\n",
      "30\n",
      "ipdb> word2ind[格]\n",
      "*** NameError: name '格' is not defined\n",
      "ipdb> word2ind['格']\n",
      "31\n",
      "ipdb> q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f9f158aae48>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/queues.py\", line 345, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-97abb248ab01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# forward + backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-97abb248ab01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# forward + backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "import time\n",
    "for key in sub_set:\n",
    "    if ii == 10:\n",
    "        break\n",
    "    ii += 1\n",
    "    net = Net(pretrained, len(pretrained))\n",
    "    cur_max_acc = 0.0\n",
    "    print(net)\n",
    "    from torch import optim\n",
    "    criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=0.01)#, momentum=0.9)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(han_x, sub_set[key], test_size=0.01, random_state=42)\n",
    "    trainset = [(train_x[i], train_y[i]) for i in range(len(train_x))]\n",
    "    testset = [(test_x[i], test_y[i]) for i in range(len(test_x))]\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                        trainset, \n",
    "                        batch_size=4,\n",
    "                        shuffle=True, \n",
    "                        num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                        testset, \n",
    "                        batch_size=4,\n",
    "                        shuffle=False, \n",
    "                        num_workers=2)\n",
    "    torch.set_num_threads(8)\n",
    "    for epoch in range(20):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            # 输入数据\n",
    "            inputs, labels = data\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward \n",
    "            import ipdb;ipdb.set_trace()\n",
    "            va = [[word2ind[key]]]\n",
    "            va = torch.from_numpy(np.array(va))\n",
    "            outputs = net(inputs, va)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()   \n",
    "\n",
    "            # 更新参数 \n",
    "            optimizer.step()\n",
    "\n",
    "            # 打印log信息\n",
    "            # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199: # 每200个batch打印一下训练状态\n",
    "                print('[%d, %5d] loss: %.3f' \\\n",
    "                      % (epoch+1, i+1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            correct, total = 0, 0\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                import ipdb;ipdb.set_trace()\n",
    "                va = [[word2ind[key]]]\n",
    "                va = torch.from_numpy(np.array(va))\n",
    "                outputs = net(images,va)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                #import ipdb;ipdb.set_trace()\n",
    "                #res = res + [map_[int(predicted[i])] for i in range(len(predicted))]\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "            if cur_max_acc < float(100 * correct / total):\n",
    "                print('-------------cur max is ::::----------'+str(float(100 * correct / total)))\n",
    "                cur_max_acc = float(100 * correct / total)\n",
    "                torch.save(net, 'charresult/net' + key + '.pkl')  # 保存整个网络\n",
    "            print('result of epoch ' + str(epoch) + 'accuracy is : %d %%' % (100 * correct / total))\n",
    "\n",
    "    print('key' + key)\n",
    "    #net2 = torch.load('net.pkl')\n",
    "    #prediction = net2(x)\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价格\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置\n",
      "操控\n",
      "舒适性\n",
      "油耗\n",
      "动力\n",
      "内饰\n",
      "安全性\n",
      "空间\n",
      "外观\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "result_set = {}\n",
    "testset = han_x_test\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "                    testset, \n",
    "                    batch_size=4,\n",
    "                    shuffle=False, \n",
    "                    num_workers=2)\n",
    "for key in sub_set:\n",
    "    print(key)\n",
    "    res = []\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for data in testloader:\n",
    "            images = data\n",
    "            va = [[word2ind[key]]]\n",
    "            va = torch.from_numpy(np.array(va))\n",
    "            net = torch.load('net++' + key + '.pkl')\n",
    "            outputs = net(images,va)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            res = res + [map_[int(predicted[i])] for i in range(len(predicted))]\n",
    "    result_set[key] = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价格1273\n",
      "1014\n",
      "145\n",
      "114\n",
      "配置853\n",
      "579\n",
      "154\n",
      "120\n",
      "操控1036\n",
      "606\n",
      "124\n",
      "306\n",
      "舒适性931\n",
      "564\n",
      "256\n",
      "111\n",
      "油耗1082\n",
      "793\n",
      "138\n",
      "151\n",
      "动力2732\n",
      "1970\n",
      "378\n",
      "384\n",
      "内饰536\n",
      "271\n",
      "150\n",
      "115\n",
      "安全性573\n",
      "380\n",
      "93\n",
      "100\n",
      "空间442\n",
      "221\n",
      "67\n",
      "154\n",
      "外观489\n",
      "263\n",
      "111\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/df_sen_sub/train.csv')\n",
    "uni_sub = data['subject'].unique()\n",
    "for key in uni_sub:\n",
    "    print(key + str(len(data[data['subject'] == key])))\n",
    "    ttt = data[data['subject'] == key]\n",
    "    print(len(ttt[ttt['sentiment_value'] == 0]))\n",
    "    print(len(ttt[ttt['sentiment_value'] == -1]))\n",
    "    print(len(ttt[ttt['sentiment_value'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_pred = {}\n",
    "content_ids = []\n",
    "subjects = []\n",
    "sentiment_values = []\n",
    "sentiment_words = []\n",
    "contents = []\n",
    "#content_id,subject,sentiment_value,sentiment_word\n",
    "for i in range(len(test_data)):\n",
    "    content_id = test_data.loc[i, 'content_id']\n",
    "    flag = 0\n",
    "    for key in result_set:\n",
    "        if result_set[key][i] != '2':\n",
    "            flag = 1\n",
    "            content_ids.append(str(content_id))\n",
    "            subjects.append(key)\n",
    "            sentiment_values.append(int(result_set[key][i]))\n",
    "            sentiment_words.append(\"\")\n",
    "            contents.append(str(test_data.loc[i, 'content']))\n",
    "    if flag == 0:\n",
    "        content_ids.append(str(content_id))\n",
    "        subjects.append('动力')\n",
    "        sentiment_values.append(0)\n",
    "        sentiment_words.append(\"\")   \n",
    "        contents.append(str(test_data.loc[i, 'content']))\n",
    "real_pred = {\"content_id\":content_ids, \"subject\":subjects, \"sentiment_value\":sentiment_values, \"sentiment_word\":sentiment_words}\n",
    "real_df = pd.DataFrame(real_pred)\n",
    "real_df.to_csv('final_result++.csv', encoding = \"utf-8\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2628, 2364)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_df), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_pred['content'] = contents\n",
    "\n",
    "dtt = pd.DataFrame(real_pred)\n",
    "dtt.to_csv('see_final_result++.csv', encoding = \"utf-8\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dtt[dtt['sentiment_value'] == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yxvenv]",
   "language": "python",
   "name": "conda-env-yxvenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
