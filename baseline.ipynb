{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision as tv\n",
    "import jieba\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch\n",
    "show = ToPILImage() # 可以把Tensor转成Image，方便可视化\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/df_sen_sub/train.csv')\n",
    "data = data.fillna('')\n",
    "senti = list(data['sentiment_value'])\n",
    "subs = list(data['subject'])\n",
    "content = list(data['content'])\n",
    "test_data = pd.read_csv('../../data/df_sen_sub/test_public.csv')\n",
    "test_data = test_data.fillna('')\n",
    "test_content = list(test_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1616, 1670, 6661)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data['sentiment_value'] == -1]), len(data[data['sentiment_value'] == 1]), len(data[data['sentiment_value'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.961 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/df_sen_sub/add_text.txt','w') as f:\n",
    "    f.write('\\n'.join(list(content) + list(test_content)  + list(subs)))\n",
    "# 迭代器，使用jieba将句子进行分词\n",
    "class Sentences(object):# 这个类可以根据实际情况重写，我已经将所有的文章进行分句，并整合到了一个文件里面\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname # 句子所在文件，没句句子占一行\n",
    "        #jieba.load_userdict(\"wordBase.txt\") # 加载词库\n",
    "\n",
    "    def __iter__(self):\n",
    "        #for fname in os.listdir(self.dirname):\n",
    "        for line in open(self.dirname):\n",
    "                line = line.replace('\\n', '')\n",
    "                yield list(jieba.cut(line))\n",
    "\n",
    "sentences = []\n",
    "def train_word2vec(folder_path, size=100):\n",
    "    global sentences\n",
    "    sentences = Sentences(folder_path) #生成分词后的句子，是一个二维数组\n",
    "\n",
    "    # size是词向量长度\n",
    "    # worker是线程数量，建议与物理线程数量一致\n",
    "    # min_count是指出现次数小于一定程度，就忽略，0表示不忽略\n",
    "    #model = Word2Vec(sentences, size=size, workers=8, min_count=0)\n",
    "\n",
    "    # 训练结束就将模型保存起来\n",
    "    #model.save(\"../../data/df_sen_sub/add_word2vec_model\")\n",
    "\n",
    "# 生成50维度的词向量模型\n",
    "train_word2vec(\"../../data/df_sen_sub/add_text.txt\",100)\n",
    "\n",
    "# 测试训练好的词向量模型，使用model[keyWord]即可获取keyword这个词的词向量\n",
    "model = Word2Vec.load(\"../../data/df_sen_sub/add_word2vec_model\")\n",
    "sentences = list(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9947, 50, 100) (2364, 50, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9947,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vecs = []\n",
    "x_test_vecs = []  \n",
    "ind = 0\n",
    "word2ind = {}\n",
    "pretrained = []\n",
    "senti_sen = []\n",
    "senti_test_sen = []\n",
    "for i in range(len(sentences) - len(content)):\n",
    "    if i < len(content):\n",
    "        temp = []\n",
    "        t_s = []\n",
    "        for j in range(len(sentences[i])):\n",
    "            if sentences[i][j] not in word2ind:\n",
    "                pretrained.append(model[sentences[i][j]])\n",
    "                word2ind[sentences[i][j]] = ind\n",
    "                ind += 1\n",
    "            t_s.append(sentences[i][j])\n",
    "            temp.append(model[sentences[i][j]])\n",
    "        while len(temp) < 50:\n",
    "            temp.append([0.0] * 100)\n",
    "        if len(temp) > 50:\n",
    "            temp = temp[:50]\n",
    "        x_vecs.append(temp)\n",
    "        senti_sen.append(t_s[:50])\n",
    "    else:\n",
    "        temp = []\n",
    "        t_s = []\n",
    "        for j in range(len(sentences[i])):\n",
    "            if sentences[i][j] not in word2ind:\n",
    "                pretrained.append(model[sentences[i][j]])\n",
    "                word2ind[sentences[i][j]] = ind\n",
    "                ind += 1\n",
    "            t_s.append(sentences[i][j])\n",
    "            temp.append(model[sentences[i][j]])\n",
    "        while len(temp) < 50:\n",
    "            temp.append([0.0] * 100)\n",
    "        if len(temp) > 50:\n",
    "            temp = temp[:50]\n",
    "        x_test_vecs.append(temp)\n",
    "        senti_test_sen.append(t_s[:50])     \n",
    "x_vecs = np.array(x_vecs)\n",
    "x_test_vecs = np.array(x_test_vecs)\n",
    "print(x_vecs.shape, x_test_vecs.shape)\n",
    "\n",
    "y_map = []\n",
    "map_ = [0, -1, 1]\n",
    "y = list(data['sentiment_value'])\n",
    "for i in range(len(y)):\n",
    "    y_map.append(map_.index(y[i]))\n",
    "y_map = np.array(y_map)\n",
    "y_map.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_vecs\n",
    "X_test = x_test_vecs\n",
    "y = y_map\n",
    "pretrained = pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pretrained, open('../../data/pretrained.txt', 'wb'))\n",
    "pickle.dump(word2ind, open('../../data/word2ind.txt', 'wb'))\n",
    "pickle.dump(senti_test_sen, open('../../data/senti_test_sen.txt', 'wb'))\n",
    "pickle.dump(senti_sen, open('../../data/senti_sen.txt', 'wb'))\n",
    "pickle.dump(X, open('../../data/sentiment_x.txt', 'wb'))\n",
    "pickle.dump(X_test, open('../../data/sentiment_x_test.txt', 'wb'))\n",
    "pickle.dump(y, open('../../data/sentiment_y.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "word2ind = pickle.load(open('../../data/word2ind.txt', 'rb'))\n",
    "senti_test_sen = pickle.load(open('../../data/senti_test_sen.txt', 'rb'))\n",
    "senti_sen = pickle.load(open('../../data/senti_sen.txt', 'rb'))\n",
    "X = pickle.load(open('../../data/sentiment_x.txt', 'rb'))\n",
    "X_test = pickle.load(open('../../data/sentiment_x_test.txt', 'rb'))\n",
    "y = pickle.load(open('../../data/sentiment_y.txt', 'rb'))\n",
    "pretrained = pickle.load(open('../../data/pretrained.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "安全性 1333\n",
      "空间 2102\n",
      "油耗 731\n",
      "舒适性 1156\n",
      "内饰 1220\n",
      "外观 2203\n",
      "动力 805\n",
      "操控 1427\n",
      "配置 294\n",
      "价格 18\n"
     ]
    }
   ],
   "source": [
    "set_subs = list(set(subs))\n",
    "va = []\n",
    "for i in range(len(set_subs)):\n",
    "    print(set_subs[i], word2ind[set_subs[i]])\n",
    "    va.append([word2ind[set_subs[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.append([0.0] * 100)\n",
    "word2ind['null'] = 17387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = np.array(pretrained)\n",
    "han_x = []\n",
    "for i in range(len(senti_sen)):\n",
    "    temp = []\n",
    "    for j in range(len(senti_sen[i])):\n",
    "        temp.append(word2ind[senti_sen[i][j]])\n",
    "    while len(temp) < 50:\n",
    "        temp.append(17387)\n",
    "\n",
    "    temp = temp[:50]\n",
    "\n",
    "    han_x.append(temp)\n",
    "han_x = np.array(han_x)\n",
    "\n",
    "han_x_test = []\n",
    "for i in range(len(senti_test_sen)):\n",
    "    temp = []\n",
    "    for j in range(len(senti_test_sen[i])):\n",
    "        temp.append(word2ind[senti_test_sen[i][j]])\n",
    "    while len(temp) < 50:\n",
    "        temp.append(17387)\n",
    "\n",
    "    temp = temp[:50]\n",
    "\n",
    "    han_x_test.append(temp)\n",
    "han_x = np.array(han_x)\n",
    "han_x_test = np.array(han_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(han_x, y, test_size=0.2, random_state=42)\n",
    "#train_x = han_x\n",
    "#train_y = y\n",
    "#test_x = han_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7957, 7957, 1990)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x), len(train_y), len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [(train_x[i], train_y[i]) for i in range(len(train_x))]\n",
    "testset = [(test_x[i], test_y[i]) for i in range(len(test_x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "                    trainset, \n",
    "                    batch_size=4,\n",
    "                    shuffle=True, \n",
    "                    num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "                    testset, \n",
    "                    batch_size=4,\n",
    "                    shuffle=False, \n",
    "                    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((50, 1)).reshape((1,2,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (bilstm): LSTM(100, 100, bidirectional=True)\n",
      "  (fch): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fcv): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fcw): Linear(in_features=200, out_features=1, bias=True)\n",
      "  (fcwp): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fcwh): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (fc1): Linear(in_features=3750, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (embeddings): Embedding(19791, 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pretrained, vocab_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size = 100, hidden_size = 100, bidirectional = True) \n",
    "        #self.fc1   = nn.Linear(3750, 50) \n",
    "        #self.fc2   = nn.Linear(200, 50)\n",
    "        #self.fc3   = nn.Linear(200, 50)\n",
    "        #self.fc4   = nn.Linear(100, 3)\n",
    "        self.fctest   = nn.Linear(200, 3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, 100)\n",
    "        pretrained_weight = np.array(pretrained)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        \n",
    "    def forward(self, x, aspects): \n",
    "        x = self.embeddings(x)#batch,50,100\n",
    "        #va = self.embeddings(aspects[0])\n",
    "        #t_va = va.unsqueeze(0).expand(x.shape[0], x.shape[1], 100)\n",
    "        output, (hn, cn) =  self.bilstm(x.float())#batch,50,200\n",
    "        H, hN = output, output[:,-1,:]\n",
    "        #con = torch.cat((t_va, H), 2)\n",
    "        #t1 = F.max_pool2d(F.relu(con), (1, 4))\n",
    "        #con = t1.view(x.size()[0], -1)\n",
    "        #con = F.relu(self.fc1(con))\n",
    "        #alpha = con.unsqueeze(1)\n",
    "        #rr = torch.matmul(alpha,H).squeeze(1)\n",
    "        #yy = torch.cat((F.relu(self.fc2(rr)), F.relu(self.fc3(hN))), 1)\n",
    "        #x = self.fc4(yy)\n",
    "        x  = self.fctest(hN)\n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net(pretrained, len(pretrained))\n",
    "print(net)\n",
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "'''\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pretrained, vocab_size):\n",
    "        super(Net, self).__init__()\n",
    "        #self.rnn1 = nn.RNN(100, 100) \n",
    "        self.bilstm = nn.LSTM(input_size = 100, hidden_size = 100, bidirectional = True)\n",
    "        #self.fc1   = nn.Linear(1250, 120) \n",
    "        self.fch = nn.Linear(200, 100)\n",
    "        self.fcv = nn.Linear(100, 100)\n",
    "        self.fcw = nn.Linear(200, 1)\n",
    "        self.fcwp = nn.Linear(200,50)\n",
    "        self.fcwh = nn.Linear(200,50)\n",
    "        self.fc1   = nn.Linear(3750, 120) \n",
    "        self.fc2   = nn.Linear(120, 50)\n",
    "        self.fc3   = nn.Linear(50, 3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, 100)\n",
    "        pretrained_weight = np.array(pretrained)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        \n",
    "    def forward(self, x, aspects): \n",
    "        x = self.embeddings(x) #[4,50,100]\n",
    "        batch_size = x.shape[0]\n",
    "        N = x.shape[1]\n",
    "        output, (hn, cn) =  self.bilstm(x.float())  #[4,50,200]\n",
    "        hN = output[:,-1,:]\n",
    "        va = self.embeddings(aspects[0]) #[1, 100]\n",
    "        t_va = torch.matmul(torch.ones((x.shape[0],50, 1)), va)#[4, 50, 100]\n",
    "        t_va = t_va.reshape((-1, 100)) #[4*50, 100]\n",
    "        x = output.reshape((-1, 200)) #[4*50, 200]\n",
    "        x = torch.tanh(torch.cat((self.fch(x), self.fcv(t_va)), 1))\n",
    "        M = x.reshape((batch_size, N, -1)) #[4, 50, 200]\n",
    "        alpha = F.softmax(self.fcw(M)).reshape((batch_size, 1, -1))#[4,50]\n",
    "        rr = torch.matmul(alpha, output).reshape((batch_size, -1)) #[4, 200]\n",
    "        final = torch.tanh(self.fcwh(hN) + self.fcwp(rr))\n",
    "        x = self.fc3(final)         \n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net(pretrained, len(pretrained))\n",
    "print(net)\n",
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)#, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/ipykernel/__main__.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.918\n",
      "[1,   400] loss: 0.865\n",
      "[1,   600] loss: 0.870\n",
      "[1,   800] loss: 0.882\n",
      "[1,  1000] loss: 0.773\n",
      "[1,  1200] loss: 0.786\n",
      "[1,  1400] loss: 0.792\n",
      "[1,  1600] loss: 0.795\n",
      "[1,  1800] loss: 0.792\n",
      "[2,   200] loss: 0.735\n",
      "[2,   400] loss: 0.797\n",
      "[2,   600] loss: 0.792\n",
      "[2,   800] loss: 0.779\n",
      "[2,  1000] loss: 0.766\n",
      "[2,  1200] loss: 0.780\n",
      "[2,  1400] loss: 0.830\n",
      "[2,  1600] loss: 0.775\n",
      "[2,  1800] loss: 0.769\n",
      "[3,   200] loss: 0.795\n",
      "[3,   400] loss: 0.717\n",
      "[3,   600] loss: 0.751\n",
      "[3,   800] loss: 0.767\n",
      "[3,  1000] loss: 0.724\n",
      "[3,  1200] loss: 0.762\n",
      "[3,  1400] loss: 0.745\n",
      "[3,  1600] loss: 0.760\n",
      "[3,  1800] loss: 0.704\n",
      "[4,   200] loss: 0.691\n",
      "[4,   400] loss: 0.717\n",
      "[4,   600] loss: 0.657\n",
      "[4,   800] loss: 0.644\n",
      "[4,  1000] loss: 0.651\n",
      "[4,  1200] loss: 0.568\n",
      "[4,  1400] loss: 0.637\n",
      "[4,  1600] loss: 0.597\n",
      "[4,  1800] loss: 0.585\n",
      "[5,   200] loss: 0.502\n",
      "[5,   400] loss: 0.445\n",
      "[5,   600] loss: 0.503\n",
      "[5,   800] loss: 0.463\n",
      "[5,  1000] loss: 0.436\n",
      "[5,  1200] loss: 0.462\n",
      "[5,  1400] loss: 0.457\n",
      "[5,  1600] loss: 0.466\n",
      "[5,  1800] loss: 0.475\n",
      "[6,   200] loss: 0.369\n",
      "[6,   400] loss: 0.379\n",
      "[6,   600] loss: 0.341\n",
      "[6,   800] loss: 0.404\n",
      "[6,  1000] loss: 0.352\n",
      "[6,  1200] loss: 0.382\n",
      "[6,  1400] loss: 0.364\n",
      "[6,  1600] loss: 0.367\n",
      "[6,  1800] loss: 0.381\n",
      "[7,   200] loss: 0.343\n",
      "[7,   400] loss: 0.308\n",
      "[7,   600] loss: 0.302\n",
      "[7,   800] loss: 0.285\n",
      "[7,  1000] loss: 0.321\n",
      "[7,  1200] loss: 0.309\n",
      "[7,  1400] loss: 0.284\n",
      "[7,  1600] loss: 0.296\n",
      "[7,  1800] loss: 0.311\n",
      "[8,   200] loss: 0.245\n",
      "[8,   400] loss: 0.256\n",
      "[8,   600] loss: 0.267\n",
      "[8,   800] loss: 0.249\n",
      "[8,  1000] loss: 0.238\n",
      "[8,  1200] loss: 0.241\n",
      "[8,  1400] loss: 0.233\n",
      "[8,  1600] loss: 0.287\n",
      "[8,  1800] loss: 0.274\n",
      "[9,   200] loss: 0.196\n",
      "[9,   400] loss: 0.211\n",
      "[9,   600] loss: 0.205\n",
      "[9,   800] loss: 0.215\n",
      "[9,  1000] loss: 0.246\n",
      "[9,  1200] loss: 0.214\n",
      "[9,  1400] loss: 0.251\n",
      "[9,  1600] loss: 0.219\n",
      "[9,  1800] loss: 0.242\n",
      "[10,   200] loss: 0.199\n",
      "[10,   400] loss: 0.203\n",
      "[10,   600] loss: 0.208\n",
      "[10,   800] loss: 0.186\n",
      "[10,  1000] loss: 0.169\n",
      "[10,  1200] loss: 0.229\n",
      "[10,  1400] loss: 0.196\n",
      "[10,  1600] loss: 0.188\n",
      "[10,  1800] loss: 0.181\n",
      "[11,   200] loss: 0.184\n",
      "[11,   400] loss: 0.158\n",
      "[11,   600] loss: 0.166\n",
      "[11,   800] loss: 0.190\n",
      "[11,  1000] loss: 0.165\n",
      "[11,  1200] loss: 0.194\n",
      "[11,  1400] loss: 0.179\n",
      "[11,  1600] loss: 0.176\n",
      "[11,  1800] loss: 0.157\n",
      "[12,   200] loss: 0.182\n",
      "[12,   400] loss: 0.138\n",
      "[12,   600] loss: 0.155\n",
      "[12,   800] loss: 0.144\n",
      "[12,  1000] loss: 0.168\n",
      "[12,  1200] loss: 0.164\n",
      "[12,  1400] loss: 0.143\n",
      "[12,  1600] loss: 0.190\n",
      "[12,  1800] loss: 0.164\n",
      "[13,   200] loss: 0.174\n",
      "[13,   400] loss: 0.140\n",
      "[13,   600] loss: 0.146\n",
      "[13,   800] loss: 0.150\n",
      "[13,  1000] loss: 0.128\n",
      "[13,  1200] loss: 0.174\n",
      "[13,  1400] loss: 0.156\n",
      "[13,  1600] loss: 0.138\n",
      "[13,  1800] loss: 0.168\n",
      "[14,   200] loss: 0.174\n",
      "[14,   400] loss: 0.140\n",
      "[14,   600] loss: 0.128\n",
      "[14,   800] loss: 0.136\n",
      "[14,  1000] loss: 0.161\n",
      "[14,  1200] loss: 0.133\n",
      "[14,  1400] loss: 0.164\n",
      "[14,  1600] loss: 0.139\n",
      "[14,  1800] loss: 0.150\n",
      "[15,   200] loss: 0.136\n",
      "[15,   400] loss: 0.136\n",
      "[15,   600] loss: 0.139\n",
      "[15,   800] loss: 0.147\n",
      "[15,  1000] loss: 0.137\n",
      "[15,  1200] loss: 0.150\n",
      "[15,  1400] loss: 0.150\n",
      "[15,  1600] loss: 0.136\n",
      "[15,  1800] loss: 0.146\n",
      "[16,   200] loss: 0.117\n",
      "[16,   400] loss: 0.147\n",
      "[16,   600] loss: 0.129\n",
      "[16,   800] loss: 0.134\n",
      "[16,  1000] loss: 0.130\n",
      "[16,  1200] loss: 0.128\n",
      "[16,  1400] loss: 0.141\n",
      "[16,  1600] loss: 0.118\n",
      "[16,  1800] loss: 0.145\n",
      "[17,   200] loss: 0.111\n",
      "[17,   400] loss: 0.100\n",
      "[17,   600] loss: 0.138\n",
      "[17,   800] loss: 0.097\n",
      "[17,  1000] loss: 0.129\n",
      "[17,  1200] loss: 0.131\n",
      "[17,  1400] loss: 0.117\n",
      "[17,  1600] loss: 0.128\n",
      "[17,  1800] loss: 0.151\n",
      "[18,   200] loss: 0.123\n",
      "[18,   400] loss: 0.092\n",
      "[18,   600] loss: 0.118\n",
      "[18,   800] loss: 0.133\n",
      "[18,  1000] loss: 0.134\n",
      "[18,  1200] loss: 0.105\n",
      "[18,  1400] loss: 0.110\n",
      "[18,  1600] loss: 0.129\n",
      "[18,  1800] loss: 0.134\n",
      "[19,   200] loss: 0.096\n",
      "[19,   400] loss: 0.101\n",
      "[19,   600] loss: 0.122\n",
      "[19,   800] loss: 0.115\n",
      "[19,  1000] loss: 0.114\n",
      "[19,  1200] loss: 0.132\n",
      "[19,  1400] loss: 0.115\n",
      "[19,  1600] loss: 0.120\n",
      "[19,  1800] loss: 0.126\n",
      "[20,   200] loss: 0.105\n",
      "[20,   400] loss: 0.097\n",
      "[20,   600] loss: 0.093\n",
      "[20,   800] loss: 0.102\n",
      "[20,  1000] loss: 0.115\n",
      "[20,  1200] loss: 0.131\n",
      "[20,  1400] loss: 0.118\n",
      "[20,  1600] loss: 0.118\n",
      "[20,  1800] loss: 0.128\n",
      "[21,   200] loss: 0.094\n",
      "[21,   400] loss: 0.082\n",
      "[21,   600] loss: 0.120\n",
      "[21,   800] loss: 0.129\n",
      "[21,  1000] loss: 0.121\n",
      "[21,  1200] loss: 0.106\n",
      "[21,  1400] loss: 0.126\n",
      "[21,  1600] loss: 0.099\n",
      "[21,  1800] loss: 0.137\n",
      "[22,   200] loss: 0.101\n",
      "[22,   400] loss: 0.116\n",
      "[22,   600] loss: 0.088\n",
      "[22,   800] loss: 0.110\n",
      "[22,  1000] loss: 0.095\n",
      "[22,  1200] loss: 0.112\n",
      "[22,  1400] loss: 0.102\n",
      "[22,  1600] loss: 0.116\n",
      "[22,  1800] loss: 0.111\n",
      "[23,   200] loss: 0.095\n",
      "[23,   400] loss: 0.110\n",
      "[23,   600] loss: 0.097\n",
      "[23,   800] loss: 0.103\n",
      "[23,  1000] loss: 0.113\n",
      "[23,  1200] loss: 0.096\n",
      "[23,  1400] loss: 0.114\n",
      "[23,  1600] loss: 0.114\n",
      "[23,  1800] loss: 0.110\n",
      "[24,   200] loss: 0.093\n",
      "[24,   400] loss: 0.102\n",
      "[24,   600] loss: 0.099\n",
      "[24,   800] loss: 0.111\n",
      "[24,  1000] loss: 0.099\n",
      "[24,  1200] loss: 0.122\n",
      "[24,  1400] loss: 0.109\n",
      "[24,  1600] loss: 0.100\n",
      "[24,  1800] loss: 0.114\n",
      "[25,   200] loss: 0.117\n",
      "[25,   400] loss: 0.065\n",
      "[25,   600] loss: 0.096\n",
      "[25,   800] loss: 0.090\n",
      "[25,  1000] loss: 0.106\n",
      "[25,  1200] loss: 0.096\n",
      "[25,  1400] loss: 0.098\n",
      "[25,  1600] loss: 0.102\n",
      "[25,  1800] loss: 0.102\n",
      "[26,   200] loss: 0.089\n",
      "[26,   400] loss: 0.112\n",
      "[26,   600] loss: 0.100\n",
      "[26,   800] loss: 0.086\n",
      "[26,  1000] loss: 0.093\n",
      "[26,  1200] loss: 0.115\n",
      "[26,  1400] loss: 0.100\n",
      "[26,  1600] loss: 0.092\n",
      "[26,  1800] loss: 0.106\n",
      "[27,   200] loss: 0.077\n",
      "[27,   400] loss: 0.103\n",
      "[27,   600] loss: 0.097\n",
      "[27,   800] loss: 0.080\n",
      "[27,  1000] loss: 0.094\n",
      "[27,  1200] loss: 0.082\n",
      "[27,  1400] loss: 0.104\n",
      "[27,  1600] loss: 0.115\n",
      "[27,  1800] loss: 0.102\n",
      "[28,   200] loss: 0.104\n",
      "[28,   400] loss: 0.094\n",
      "[28,   600] loss: 0.084\n",
      "[28,   800] loss: 0.102\n",
      "[28,  1000] loss: 0.094\n",
      "[28,  1200] loss: 0.092\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(8)\n",
    "for epoch in range(30):  \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # 输入数据\n",
    "        inputs, labels = data\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward \n",
    "        va = torch.from_numpy(np.array(va))\n",
    "        outputs = net(inputs, va)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()   \n",
    "        \n",
    "        # 更新参数 \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印log信息\n",
    "        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199: # 每200个batch打印一下训练状态\n",
    "            print('[%d, %5d] loss: %.3f' \\\n",
    "                  % (epoch+1, i+1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实际的label:         1        0        0        0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-d5a06d0b5e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'实际的label: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m            \u001b[0;34m'%08s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mmap_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 计算图片在每个类别上的分数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# 得分最高的那个类\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9a6d53cc947f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, aspects)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[4,50,100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m         return F.embedding(\n\u001b[1;32m    109\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "map_ = [0, -1, 1]\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next() # 一个batch返回4张图片q\n",
    "images = dataiter.next() # 一个batch返回4张图片\n",
    "print('实际的label: ', ' '.join(\\\n",
    "            '%08s'%map_[labels[j]] for j in range(4)))\n",
    "# 计算图片在每个类别上的分数\n",
    "outputs = net(images,va)\n",
    "# 得分最高的那个类\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print('预测结果: ', ' '.join('%5s'\\\n",
    "            % map_[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f25a9108ef0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/queues.py\", line 345, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0e1e44dca14d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;31m#images, labels = data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmap_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9a6d53cc947f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, aspects)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[4,50,100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m         return F.embedding(\n\u001b[1;32m    109\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/yxvenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "correct = 0 # 预测正确的图片数\n",
    "total = 0 # 总共的图片数\n",
    "\n",
    "\n",
    "# 由于测试的时候不需要求导，可以暂时关闭autograd，提高速度，节约内存\n",
    "fake = {}\n",
    "text = []\n",
    "vec = []\n",
    "l = []\n",
    "p = []\n",
    "res = []\n",
    "map_ = [0, -1, 1]\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images = data#images, labels = data\n",
    "        outputs = net(images,va)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        res = res + [map_[int(predicted[i])] for i in range(len(predicted))]\n",
    "        #for j in range(len(predicted)):\n",
    "            #if predicted[j] != labels[j]:\n",
    "                #vec.append(images[j])\n",
    "                #l.append(map_[int(labels[j])])\n",
    "                #p.append(map_[int(predicted[j])])\n",
    "        #total += labels.size(0)\n",
    "        #correct += (predicted == labels).sum()\n",
    "\n",
    "#print('3283张测试集中的准确率为: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2word = {}\n",
    "for key in word2ind:\n",
    "    ind2word[word2ind[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vec)):\n",
    "    temp = vec[i]\n",
    "    tt = []\n",
    "    for j in range(len(temp)):\n",
    "        tt.append(ind2word[int(temp[j])])\n",
    "    text.append(''.join(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = {}\n",
    "content_id,subject,sentiment_value,sentiment_word\n",
    "fake['text'] = test_content\n",
    "fake['sentiment_value'] = res\n",
    "df = pd.DataFrame(fake)\n",
    "df.to_csv('../../data/df_sen_sub/senti_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['predict'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yxvenv]",
   "language": "python",
   "name": "conda-env-yxvenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
